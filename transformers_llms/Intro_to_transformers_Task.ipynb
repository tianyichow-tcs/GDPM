{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/MENA-ML/tutorials2025-tasks/blob/main/transformers_llms/Intro_to_transformers_Task.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "ZAgxx0y7NUOH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXCEblP34Ch7"
      },
      "source": [
        "# Building a Transformer from Scratch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcLzSq-RbqPj"
      },
      "source": [
        "Welcome to this hands-on tutorial where you'll dive into the world of PyTorch and learn to build a [Transformer](https://arxiv.org/abs/1706.03762) model!\n",
        "\n",
        "**What is PyTorch?**\n",
        "\n",
        "PyTorch is a widely used, open-source machine learning framework known for its flexibility, ease of use, and strong support for GPU acceleration. It's favored by researchers and developers alike for building and deploying deep learning models, thanks to its dynamic computational graph, intuitive Pythonic syntax, and extensive ecosystem of tools and libraries.\n",
        "\n",
        "**Why PyTorch?**\n",
        "\n",
        "Flexibility and Control: PyTorch's dynamic computation graph allows for more intuitive model building and debugging, especially for complex architectures.\n",
        "Strong GPU Acceleration: PyTorch seamlessly integrates with GPUs, significantly speeding up training and inference.\n",
        "Pythonic and Easy to Learn: PyTorch's API is designed to be intuitive for Python developers, making it relatively easy to learn and use.\n",
        "Large and Active Community: PyTorch benefits from a vast and active community, providing ample resources, support, and pre-trained models.\n",
        "In this walk-through tutorial, you'll learn:\n",
        "\n",
        "- **Part I** : The fundamentals of PyTorch, including its tensor operations and automatic differentiation capabilities.\n",
        "How to build a basic Multilayer Perceptron (MLP) model using PyTorch.\n",
        "- **Part II** : How to build a text tokenizer.\n",
        "- **Part III** : How to implement a Transformer model from scratch, for classification tasks.\n",
        "- **Part IV** : How to implement a Transformer model from scratch, for sequence generation tasks.\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pmTZZQ1rcotF"
      },
      "outputs": [],
      "source": [
        "import functools  # Used for creating partial functions\n",
        "import numpy as np  # Used for numerical computation in plain NumPy to compare with PyTorch\n",
        "import tqdm.notebook  as tqdm # Used for displaying progress bars\n",
        "import matplotlib.pyplot as plt  # Used for plotting graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-AcU--hgcvoV"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "from typing import Any, Mapping, Tuple\n",
        "import pandas as pd\n",
        "import seaborn as sns  # Visualization library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SOIzaZdMczwd"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf  # We only import it for the tokenizer\n",
        "import torch\n",
        "import torch._dynamo\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set global torch defaults\n",
        "torch.set_default_dtype(torch.float32)\n",
        "torch.set_float32_matmul_precision('high')\n",
        "torch._dynamo.config.suppress_errors = True"
      ],
      "metadata": {
        "id": "L3DIbnldwjRl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style(\n",
        "    \"whitegrid\"\n",
        ")  # See example in https://seaborn.pydata.org/generated/seaborn.set_style.html"
      ],
      "metadata": {
        "id": "VeFNuc5SwyLl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "W7o5tDZGLvcq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20bda0e8-7fe1-4974-ed2d-a0d59c8625d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7d7cf3e550d0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "seed = 2024\n",
        "np.random.seed(seed)\n",
        "generator = torch.Generator()\n",
        "generator.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTpUJYXudMIB"
      },
      "source": [
        "In PyTorch, neural networks are typically defined as classes that inherit from `torch.nn.Module`. This object-oriented approach provides a structured way to define and manage your network's layers and parameters. For optimization, PyTorch offers a variety of optimizers within the `torch.optim` package, such as Adam, SGD, and others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KYWDBMYxda3E"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn  # PyTorch's neural network module\n",
        "import torch.optim as optim  # PyTorch's optimizer library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3blc81WeT2z"
      },
      "source": [
        "In PyTorch, you typically need to explicitly specify the device you want to use for computation (e.g., CPU or GPU).  PyTorch provides torch.device to represent devices, and `tensor.to(device)` or `module.to(device)` to move data and models to the desired device.  You can check if a GPU is available and which device your tensors are on as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rc-ohyXbeeT2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3283b0cd-a837-494b-9e55-4b70f1efcf3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device.type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ9Ymfr8b4hk"
      },
      "source": [
        "# Part I: Pytorch\n",
        "After completing this part, you will be able to:\n",
        "\n",
        "1. Understand the basics of PyTorch.\n",
        "2. Convert NumPy code to PyTorch code.\n",
        "3. Utilize PyTorch's compilation to speed up your numerical computations.\n",
        "4. Calculate gradients using PyTorch's automatic differentiation capabilities.\n",
        "5. Implement a basic training loop in PyTorch to optimize a simple model.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Things to try:\n",
        "1. Implement weight decay by adding a regularization term $||W||$ to the loss function.\n",
        "2. What happens to the training and test loss when we use more layers.\n",
        "3. What happens when the learning rate is increased/decreased?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eih_7ru5e-e7"
      },
      "source": [
        "## simple model\n",
        "Before diving into PyTorch, let's first understand the simple Multilayer Perceptron (MLP) architecture we'll be implementing. An MLP consists of multiple layers of interconnected nodes (neurons). Each connection between nodes has an associated weight, and each node applies an activation function to its input.\n",
        "\n",
        "In this Colab, we'll be working with a simple MLP with one hidden layer. The input data is first multiplied by a weight matrix W, before a bias term b is added. Then, a ReLU (Rectified Linear Unit) activation function is applied. This introduces non-linearity, allowing the model to learn more complex patterns in the data. The output of the hidden layer is then multiplied by another weight matrix and a bias term is added to produce the final output.\n",
        "\n",
        "We'll implement this MLP first using NumPy and then using PyTorch, showcasing how easy it is to transition between the two libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnK-hZYQkgwI"
      },
      "source": [
        "## fundamentals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hrknNUomVjo"
      },
      "source": [
        "### numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Use the example code below to implement your own `predict` and `loss` functions.\n",
        "def predict(params, inputs):\n",
        "  \"\"\"Numpy implementation of the MLP architecture.\n",
        "\n",
        "  Implement a function that loops through all layers and unpack them into\n",
        "  weights W and bias b.\n",
        "  Then, it does matrix multiplication followed by relu activation.\n",
        "\n",
        "  Args:\n",
        "    params: a list [(W0, b0), (W1, b1), ...] containing the neural network\n",
        "      weights.\n",
        "    inputs: the inputs to the neural network,\n",
        "      of shape (batch_size, input_dimension).\n",
        "\n",
        "  Returns:\n",
        "    outputs: the outputs of the neural network,\n",
        "      of shape (batch_size, output_dimension)\n",
        "  \"\"\"\n",
        "  outputs = []  # in case params = []\n",
        "  for W, b in params:\n",
        "\n",
        "    # --- YOUR CODE START ---\n",
        "    # outputs =... @... +...  # matrix multiplication on inputs\n",
        "    # inputs =...(...)  # relu activation\n",
        "    # --- YOUR CODE END ---\n",
        "    outputs = W @ inputs\n",
        "    out\n",
        "  return outputs\n",
        "\n",
        "def loss(params, batch):\n",
        "  \"\"\"Numpy implementation of the square loss function.\n",
        "\n",
        "  Args:\n",
        "    params: a list [(W0, b0), (W1, b1), ...] containing the neural network\n",
        "      weights.\n",
        "    batch: a tuple (inputs, targets).\n",
        "\n",
        "  Returns:\n",
        "    loss: the loss value.\n",
        "  \"\"\"\n",
        "  loss_value = 0\n",
        "  inputs, targets = batch  # Unpack the batch into inputs and targets\n",
        "  # --- YOUR CODE START ---\n",
        "  # preds =...(...)  # get the predictions\n",
        "  # loss_value = ...  # calculate the mean squared error\n",
        "  # --- YOUR CODE END ---\n",
        "\n",
        "  return loss_value"
      ],
      "metadata": {
        "id": "8yXOvw4xnwBb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kh0a88kDk4vX"
      },
      "outputs": [],
      "source": [
        "# prepare data\n",
        "# here: target (y_train) is a linear function of input (x_train) plus some noise\n",
        "\n",
        "num_examples = 10_000\n",
        "dim = 100\n",
        "x_train = np.random.randn(num_examples, dim)\n",
        "w = np.random.randn(dim,)\n",
        "y_train = np.dot(x_train, w) + 0.2 * np.random.randn(num_examples,)\n",
        "\n",
        "x_train = x_train.astype(np.float32)\n",
        "y_train = y_train.astype(np.float32)\n",
        "\n",
        "batch = (x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJZ1oxupkmt6"
      },
      "outputs": [],
      "source": [
        "# initialize model parameters\n",
        "W1 = np.identity(dim)  # identity matrix\n",
        "b1 = 0.\n",
        "\n",
        "W2 = np.random.randn(dim,)\n",
        "b2 = 0.\n",
        "\n",
        "params = [(W1, b1), (W2, b2)]  # two layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhFObBH_H7Us"
      },
      "outputs": [],
      "source": [
        "loss(params, batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3Wr76gDlIWh"
      },
      "outputs": [],
      "source": [
        "# This magic command measures the execution time of the loss function.\n",
        "%timeit loss(params, batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuN36FtgmXts"
      },
      "source": [
        "### pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now implement the same functions in pytorch\n",
        "\n",
        "def predict(params, inputs):\n",
        "  \"\"\"PyTorch implementation of the model.\"\"\"\n",
        "  outputs = None  # in case params is empty\n",
        "  for W, b in params:\n",
        "    # Note that there is no need to transpose in PyTorch, matmul handles it\n",
        "    pass\n",
        "    # --- YOUR CODE START ---\n",
        "    # outputs =... @... +...  # matrix multiplication on inputs\n",
        "    # inputs =...(...)  # relu activation\n",
        "    # --- YOUR CODE END ---\n",
        "  return outputs\n",
        "\n",
        "def loss(params, batch):\n",
        "  \"\"\"PyTorch implementation of the loss function.\"\"\"\n",
        "  loss_value = 0\n",
        "  inputs, targets = batch  # Unpack the batch into inputs and targets\n",
        "  # --- YOUR CODE START ---\n",
        "  # preds =...(...)  # get the predictions\n",
        "  # loss_value = ...  # calculate the mean squared error\n",
        "  # --- YOUR CODE END ---\n",
        "\n",
        "  return loss_value"
      ],
      "metadata": {
        "id": "K30xBxfapW_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mp6JgOw2mZS5"
      },
      "outputs": [],
      "source": [
        "# Use torch.tensor() to load data into PyTorch\n",
        "\n",
        "x_train_tensor = torch.tensor(x_train)\n",
        "y_train_tensor = torch.tensor(y_train)\n",
        "\n",
        "W1_tensor = torch.tensor(W1).float()\n",
        "b1_tensor = torch.tensor(b1).float()\n",
        "W2_tensor = torch.tensor(W2).float()\n",
        "b2_tensor = torch.tensor(b2).float()\n",
        "\n",
        "batch = (x_train_tensor, y_train_tensor)\n",
        "params = [(W1_tensor, b1_tensor), (W2_tensor, b2_tensor)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIX7xHG2nG2X"
      },
      "outputs": [],
      "source": [
        "# Warm up. The result of `loss(params, batch)` is a PyTorch Tensor.\n",
        "# If a CUDA-enabled GPU is available, the tensor will likely reside on the GPU.\n",
        "\n",
        "loss(params, batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnVlTcfR2h9P"
      },
      "source": [
        "We haven't done anything yet, like compilation or parallelization. Yet, we observe a significant speedup already."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hlhPQ6Hmq_Z"
      },
      "outputs": [],
      "source": [
        "# PyTorch also benefits from just-in-time compilation and GPU acceleration.\n",
        "# The first run might be slower due to initial overhead, but subsequent runs are usually faster.\n",
        "# The `%timeit` magic command in IPython/Jupyter will automatically run the function multiple times\n",
        "# to give you an average execution time.\n",
        "\n",
        "%timeit loss(params, batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmmHnp6Onii9"
      },
      "source": [
        "### jit\n",
        "When using Just-in-time (JIT), the code is complied the first time, then the compiled code is used in the subsequent calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KE-9A83nkDL"
      },
      "outputs": [],
      "source": [
        "# Compiling using torch.compile\n",
        "\n",
        "jit_loss = torch.compile(loss)\n",
        "\n",
        "jit_loss(params, batch)  # warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0To6ftKoY7m"
      },
      "outputs": [],
      "source": [
        "# now compare the time with before. We bigger speedup without having to change\n",
        "# much in our code\n",
        "%timeit jit_loss(params, batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m10dBKiBt9G9"
      },
      "source": [
        "### auto-differentiation\n",
        "\n",
        "Automatic differentiation is a technique for automatically calculating the gradients of a function. This is crucial for training neural networks, where we need to know the gradient of the loss function with respect to the model parameters in order to update the parameters and improve the model.\n",
        "\n",
        "In PyTorch, automatic differentiation is handled by the torch.autograd package. It works by dynamically building a computational graph as you perform operations on tensors that have `requires_grad=True`.  Gradients are then calculated using the chain rule during the backward pass, which is initiated by calling `.backward()` on a scalar tensor (usually the loss).\n",
        "\n",
        "Think of it like this: PyTorch's autograd keeps track of operations on tensors that require gradients. When you call .`backward()`, it automatically traverses this recorded history to compute the gradients for you. You can then access these gradients through the `.grad` attribute of your tensors."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_loss(params, batch):\n",
        "  \"\"\"Calculates the gradient of the loss function w.r.t. the parameters.\"\"\"\n",
        "  for W, b in params:\n",
        "    pass\n",
        "    # --- YOUR CODE START ---\n",
        "    #... # set requires_grad=True for the parameters\n",
        "    # --- YOUR CODE END ---\n",
        "\n",
        "  loss_value = jit_loss(params, batch) # Use the traced loss for efficiency\n",
        "  # --- YOUR CODE START ---\n",
        "  # ... # calculate gradients using autograd\n",
        "  # --- YOUR CODE END ---\n",
        "\n",
        "  # Collect gradients\n",
        "  grads = []\n",
        "  for W, b in params:\n",
        "    pass\n",
        "    # --- YOUR CODE START ---\n",
        "    # grads.append(...)  # collect the gradients\n",
        "    #... # Set requires_grad back to False and reset\n",
        "    # --- YOUR CODE END ---\n",
        "  return grads\n",
        "\n",
        "# Because params is a list of tuples, the gradient will also be a list of tuples\n",
        "print(grad_loss(params, batch))"
      ],
      "metadata": {
        "id": "N8Vz-FNsqRcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyUBWKfmvJlm"
      },
      "source": [
        "## Training Loop: putting it all together\n",
        "This training loop iteratively updates the model parameters to minimize the loss function. Here's a breakdown of the steps involved:\n",
        "\n",
        "1. **Calculate the loss:** The `jit_loss` function calculates the loss between the model's predictions and the actual targets.\n",
        "2. **Calculate the gradient:** The `grad_loss` function calculates the gradient of the loss function with respect to the model parameters. This tells us how to adjust the parameters to reduce the loss.\n",
        "3. **Update the parameters:** The parameters are updated by subtracting a fraction of the gradient (determined by the learning rate `lr`) from the current parameter values. This moves the parameters in the direction that reduces the loss.\n",
        "\n",
        "This process is repeated for a specified number of steps (`num_steps`), gradually improving the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmYGds1lvdmh"
      },
      "outputs": [],
      "source": [
        "num_steps = 500  # total number of steps\n",
        "lr = 0.001  # learning rate\n",
        "\n",
        "modelparams = params  # initialize the parameters\n",
        "history = []\n",
        "\n",
        "# Use a progress bar from tqdm\n",
        "for i in tqdm.tqdm(range(num_steps), desc=\"Training Progress\"):\n",
        "  # calculate the loss\n",
        "  new_loss = jit_loss(modelparams, batch)\n",
        "  history.append(new_loss.item()) # Append the loss value as a Python scalar\n",
        "\n",
        "  # calculate the gradient\n",
        "  grads = grad_loss(modelparams, batch)\n",
        "\n",
        "  # update the parameters\n",
        "  with torch.no_grad():  # Disable gradient tracking during updates\n",
        "      for lyr in range(len(modelparams)):\n",
        "          modelparams[lyr] = (\n",
        "              modelparams[lyr][0] - lr * grads[lyr][0],\n",
        "              modelparams[lyr][1] - lr * grads[lyr][1],\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ybGnf0LBk3A"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))  # Adjust figure size for better readability\n",
        "plt.plot(history, color='blue', linewidth=2)  # Customize line color and width\n",
        "plt.title('Training Loss', fontsize=14)  # Add a title with increased font size\n",
        "plt.xlabel('Training Step', fontsize=12)  # Add x-axis label with increased font size\n",
        "plt.ylabel('Loss (Log Scale)', fontsize=12)  # Add y-axis label with increased font size\n",
        "plt.yscale('log')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)  # Add a grid for better readability\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB7pn7K8b7FW"
      },
      "source": [
        "# Part II: Text Tokenization\n",
        "\n",
        "Part II provides a gentle introduction to tokenization, a fundamental step in Natural Language Processing (NLP). Tokenization is the process of breaking down text into smaller units, called tokens, which can be words, subwords, or characters. These tokens are then converted into numerical representations that can be processed by machine learning models.\n",
        "\n",
        "Our \"difference dataset\" is of the form `<number1 - number2>`, where `number1` and `number2` are some integers and the binary label y is 1 if and only if `number1 > number2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnS_q3k35f4I"
      },
      "outputs": [],
      "source": [
        "# @title dataset\n",
        "# @markdown <font color='blue'>Double click to see how the dataset is constructed</font>\n",
        "def sample_difference_dataset(\n",
        "    dataset_size: int,\n",
        "    lengths: list[int],\n",
        "    k: int,\n",
        "):\n",
        "  \"\"\"Generates a dataset of expressions representing the difference between two\n",
        "\n",
        "  numbers.\n",
        "\n",
        "  Args:\n",
        "    dataset_size: The number of expressions to generate.\n",
        "    lengths: A list of integers representing the desired lengths (number of\n",
        "      digits) of the expressions. For example, the expression 123 - 4 has a\n",
        "      length of five.\n",
        "    k: The base of the numbers in the expressions.\n",
        "\n",
        "  Returns:\n",
        "    A list of tuples, where each tuple contains an expression (str) and\n",
        "    its corresponding sign (1 if the difference is positive, 0 otherwise).\n",
        "  \"\"\"\n",
        "  data_all = []\n",
        "  for length in lengths:\n",
        "    if length <= 2:\n",
        "      raise ValueError(\n",
        "          f\"The length of the expression must be greater than 2, got {length}.\"\n",
        "      )\n",
        "\n",
        "    # We only use `length - 1` tokens for the two values to account for the `+`.\n",
        "    length_n = np.random.randint(1, length - 1, size=(dataset_size,))\n",
        "    length_m = length - 1 - length_n\n",
        "\n",
        "    integer_n = [random.randint(1, k ** int(len_n) - 1) for len_n in length_n]\n",
        "    integer_m = [random.randint(1, k ** int(len_m) - 1) for len_m in length_m]\n",
        "    diff_sign = [int(x > y) for x, y in zip(integer_n, integer_m)]\n",
        "\n",
        "    integer_n = [str(x)[::-1] for x in integer_n]\n",
        "    integer_m = [str(x)[::-1] for x in integer_m]\n",
        "    expressions = [f\"{a}-{b}\" for a, b in zip(integer_n, integer_m)]\n",
        "\n",
        "    data = [(x, y) for x, y in zip(expressions, diff_sign)]\n",
        "    data_all.extend(data)\n",
        "  data_all = list(set(data_all))\n",
        "  random.shuffle(data_all)\n",
        "  return data_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRGWOFT06PiF"
      },
      "outputs": [],
      "source": [
        "MAX_TRAIN_LENGTH = 10  # the maximum length allowed in the training split\n",
        "MAX_TEST_LENGTH = 15  # the maximum length allowed in the test split\n",
        "\n",
        "train_ds = sample_difference_dataset(\n",
        "    dataset_size=2500,\n",
        "    lengths=list(range(3, MAX_TRAIN_LENGTH + 1)),\n",
        "    k=10,\n",
        ")\n",
        "test_ds = sample_difference_dataset(\n",
        "    dataset_size=1000,\n",
        "    lengths=list(range(MAX_TRAIN_LENGTH + 1, MAX_TEST_LENGTH + 1)),\n",
        "    k=10,\n",
        ")\n",
        "\n",
        "print(f\"Train dataset size {len(train_ds)}\")\n",
        "print(f\"Test dataset size {len(test_ds)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FktMlN_lNObR"
      },
      "source": [
        "First, let's see how a single example in the dataset looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWfWM5FjKMNn"
      },
      "outputs": [],
      "source": [
        "it = iter(train_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ-ow2NLMdtd"
      },
      "outputs": [],
      "source": [
        "# let's see a few examples\n",
        "for _ in range(5):\n",
        "  text, label = next(it)\n",
        "  print(\"text: \", text)\n",
        "  print(\"label: \", label)\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW1QoP1C1VDo"
      },
      "source": [
        "## building the vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivbM47VmfzNs"
      },
      "source": [
        "In order to build a tokenizer, we will need to build a vocabulary of tokens (think of those as 'words' or any other useful chunks of text). To construct a good vocabulary, we will learn it from our dataset itself. So, let's collect a bunch of examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3fyknN8ggGQ"
      },
      "outputs": [],
      "source": [
        "# size of corpus to build the tokenizer\n",
        "corpus_size = 5_000  # @param = 'int'\n",
        "\n",
        "# size of the vocabulary\n",
        "vocab_size = 12  # @param = 'int'\n",
        "\n",
        "# maximum length of examples in tokens\n",
        "max_len = MAX_TEST_LENGTH + 1  # @param = 'int'\n",
        "\n",
        "# pad value\n",
        "pad_value = 0  # @param = 'int'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22kG9DYXf1jk"
      },
      "outputs": [],
      "source": [
        "#  preprocessing function will decode and keep text only\n",
        "corpus = [text for text, _ in train_ds[:corpus_size]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJqOiYXdcfJ6"
      },
      "outputs": [],
      "source": [
        "corpus[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JaZvhsV1dxg"
      },
      "source": [
        "## creating the tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIsA18cJgDhZ"
      },
      "outputs": [],
      "source": [
        "# now, we build the tokenizer\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "    num_words=vocab_size,\n",
        "    oov_token=None,\n",
        "    char_level=True,\n",
        ")\n",
        "tokenizer.fit_on_texts(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Tsoh25H7jFb"
      },
      "outputs": [],
      "source": [
        "# note how the tokenizer figured out it was best to tokenize each digit separately\n",
        "tokenizer.index_word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7rDeh97g3l6"
      },
      "source": [
        "The tokenizer is now trained. Let's see how it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-7TYV31MvPW"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "print(\"original text: \", text)\n",
        "\n",
        "# tokenize text\n",
        "tokens = tokenizer.texts_to_sequences([text])\n",
        "print(\"tokens: \", tokens)\n",
        "print(\"number of tokens: \", len(tokens[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1q04rbbxota"
      },
      "outputs": [],
      "source": [
        "# we can see the actual tokens by converting each token individually to text\n",
        "print(tokenizer.sequences_to_texts(np.array(tokens).reshape((-1, 1))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYZM041p39Jt"
      },
      "source": [
        "get some information about the distribution of tokens in our corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlpzUoXz3ERI"
      },
      "outputs": [],
      "source": [
        "# Let's examine the distribution of tokens in the corpus:\n",
        "print(\"Token frequency:\")\n",
        "dict(list(tokenizer.word_counts.items()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17d1OZQv1kDR"
      },
      "source": [
        "## preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVDv4zWRwq0w"
      },
      "source": [
        "The last step is to make sure that all examples have the same shape so that they can be batched together. For that, we will need to pad short examples with a special padding token. Second, we will use the last token as `cls` for classification. We can give it a special value or just use the same as the padding value. We will use a value for 0 for both.\n",
        "\n",
        "**Why?**\n",
        "\n",
        "Neural networks typically process data in batches for efficiency. Batching allows the network to perform computations on multiple examples simultaneously, which speeds up training and inference. However, for batching to work, all examples in a batch must have the same shape (i.e., the same number of tokens).\n",
        "\n",
        "Since text sequences have variable lengths, we use padding to make them uniform. Padding involves adding special padding tokens to shorter sequences to make them the same length as the longest sequence in the batch. This ensures that all examples in a batch have a consistent shape, enabling efficient batch processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CO1DggBTtY5I"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(text, label):\n",
        "  pass\n",
        "  # --- YOUR CODE START ---\n",
        "  # tokens =...  # tokenize the text\n",
        "  # tokens =...  # pad the sequences\n",
        "  # tokens =...  # convert to tensor\n",
        "  # label =...  # convert to tensor\n",
        "  # --- YOUR CODE END ---\n",
        "  return tokens, label"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the preprocessing function to the training and test datasets\n",
        "print(\"preprocessing training examples ... \")\n",
        "x_train = []\n",
        "y_train = []\n",
        "for text, label in tqdm.tqdm(train_ds):\n",
        "  tokens, label = preprocess_function(text, label)\n",
        "  x_train.append(tokens)\n",
        "  y_train.append(label)\n",
        "\n",
        "print(\"preprocessing test examples ... \")\n",
        "x_test = []\n",
        "y_test = []\n",
        "for text, label in tqdm.tqdm(test_ds):\n",
        "  tokens, label = preprocess_function(text, label)\n",
        "  x_test.append(tokens)\n",
        "  y_test.append(label)\n",
        "\n",
        "# convert to Jax Arrays\n",
        "x_train = torch.stack(x_train)\n",
        "y_train = torch.cat(y_train)\n",
        "x_test = torch.stack(x_test)\n",
        "y_test = torch.cat(y_test)"
      ],
      "metadata": {
        "id": "--Mnf-QoO5--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Jd4JuiKdQzp"
      },
      "outputs": [],
      "source": [
        "print(\"x_train.shape: \", x_train.shape)\n",
        "print(\"y_train.shape: \", y_train.shape)\n",
        "print(\"x_test.shape: \", x_test.shape)\n",
        "print(\"y_test.shape: \", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4g7FPXRLxauC"
      },
      "outputs": [],
      "source": [
        "# let's see how it looks like\n",
        "x_train[0], y_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRnmakhtB8Ru"
      },
      "source": [
        "# Part III: Transformer Architecture - Classification task\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop"
      ],
      "metadata": {
        "id": "DeidnGmA9MsE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kywVHJr3CS7T"
      },
      "source": [
        "First, let's define and fix our loss function, training step and training loop. We won't need to change these later. Make sure you execute the cell below and double click to see the actual code."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <font color=\"blue\">Double click here to see the training loop</font>\n",
        "class TrainState:\n",
        "    def __init__(self, model: nn.Module, optimizer: Optimizer, step: int = 0, **kwargs: Any):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.step = step\n",
        "        self.metadata = kwargs\n",
        "\n",
        "    def state_dict(self):\n",
        "        return {\n",
        "            \"step\": self.step,\n",
        "            \"model_state\": self.model.state_dict(),\n",
        "            \"optimizer_state\": self.optimizer.state_dict(),\n",
        "            \"metadata\": self.metadata,\n",
        "        }\n",
        "\n",
        "def train(Model, epochs=10, batch_size=128, lr=3e-4, wd=1e-5, **kwargs):\n",
        "    # Initialize the model\n",
        "    model = Model(**kwargs)  # 2 classes for sentiment analysis\n",
        "\n",
        "    # Use GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Initialize the optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "    # Create a TrainState equivalent (using a class as defined previously)\n",
        "    state = TrainState(model=model, optimizer=optimizer)\n",
        "\n",
        "    # Define the loss function\n",
        "    def loss_fn(params, x, y):\n",
        "        # params are automatically handled by PyTorch within the model\n",
        "        model.load_state_dict(params) # Load the parameters into the model\n",
        "\n",
        "        logits = model(x)\n",
        "\n",
        "        loss = nn.CrossEntropyLoss()(logits, y)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    # Gradient function (using autograd)\n",
        "    def train_step(state, x, y):\n",
        "        # Enable gradient calculation\n",
        "        state.model.train()\n",
        "\n",
        "        loss = loss_fn(state.model.state_dict(), x, y)\n",
        "\n",
        "        # Calculate gradients\n",
        "        state.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients\n",
        "        torch.nn.utils.clip_grad_norm_(state.model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Update parameters\n",
        "        state.optimizer.step()\n",
        "\n",
        "        return state, loss.item()\n",
        "\n",
        "    # Report accuracy and loss\n",
        "    def report(state, x, y):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        state.model.eval()  # Set the model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            logits = state.model(x)\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "            acc = torch.sum(predictions == y).item()\n",
        "        return acc\n",
        "\n",
        "    train_step_jit = torch.compile(train_step)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_dataset = TensorDataset(x_train, y_train)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    test_dataset = TensorDataset(x_test, y_test)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Training accuracy (on a small subset only)\n",
        "    num_eval_examples = min(3200, len(x_train))\n",
        "    train_accuracy = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, num_eval_examples, batch_size):\n",
        "            x_batch = x_train[i : i + batch_size].to(device)\n",
        "            y_batch = y_train[i : i + batch_size].to(device)\n",
        "            train_accuracy += report(state, x_batch, y_batch)\n",
        "    train_accuracy /= num_eval_examples\n",
        "\n",
        "    # Test accuracy\n",
        "    num_eval_examples = min(3200, len(x_test))\n",
        "    test_accuracy = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, num_eval_examples, batch_size):\n",
        "            x_batch = x_test[i : i + batch_size].to(device)\n",
        "            y_batch = y_test[i : i + batch_size].to(device)\n",
        "            test_accuracy += report(state, x_batch, y_batch)\n",
        "    test_accuracy /= num_eval_examples\n",
        "\n",
        "    print(\"Before training:\")\n",
        "    print(f\"train accuracy: {train_accuracy}, test accuracy: {test_accuracy}\")\n",
        "\n",
        "    # Begin the training loop\n",
        "    print(f\"epochs {epochs}\")\n",
        "    for epoch in tqdm.tqdm(range(epochs), desc=\"Epochs\"):\n",
        "      model.train()\n",
        "      for x_batch, y_batch in tqdm.tqdm(train_loader, desc=\"Batches\", leave=False):\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "        state, loss = train_step_jit(state, x_batch, y_batch)\n",
        "\n",
        "        # Training accuracy\n",
        "        train_accuracy = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(x_train), batch_size):\n",
        "                x_batch = x_train[i : i + batch_size].to(device)\n",
        "                y_batch = y_train[i : i + batch_size].to(device)\n",
        "\n",
        "                train_accuracy += report(state, x_batch, y_batch)\n",
        "        train_accuracy /= len(x_train)\n",
        "\n",
        "        # Test accuracy\n",
        "      test_accuracy = 0\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "        for i in range(0, len(x_test), batch_size):\n",
        "          x_batch = x_test[i : i + batch_size].to(device)\n",
        "          y_batch = y_test[i : i + batch_size].to(device)\n",
        "          test_accuracy += report(state, x_batch, y_batch)\n",
        "      test_accuracy /= len(x_test)\n",
        "\n",
        "      print(f\"Epoch: {epoch + 1}\")\n",
        "      print(f\"train accuracy: {train_accuracy}, test accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "id": "LzmKPUZvttC1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9BDjAOECELU"
      },
      "source": [
        "## initial model\n",
        "Initially, we will only use a single self-attention layer and then apply a linear classifier on the CLS token. Later, we will improve the model step-by-step until we arrive at the standard transformer architecture."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the self-attention layer\n",
        "class SelfAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_dim):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "\n",
        "    # Calculate query, key, and value matrices using linear layers\n",
        "    self.query = nn.Linear(embed_dim, embed_dim)\n",
        "    self.key = nn.Linear(embed_dim, embed_dim)\n",
        "    self.value = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # --- YOUR CODE START ---\n",
        "    query = self.query(x)\n",
        "    key = self.key(x)\n",
        "    value = self.value(x)\n",
        "    # attention_scores =...  # calculate the attention scores\n",
        "    # attention_weights =...  # apply the softmax function\n",
        "    # output =...  # apply the attention weights\n",
        "    # --- YOUR CODE END ---\n",
        "    return output\n",
        "\n",
        "\n",
        "# Define the model\n",
        "class SimpleTransformer(nn.Module):\n",
        "\n",
        "  def __init__(\n",
        "      self, vocab_size, embed_dim=128, num_classes=2, max_seq_length=None\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.num_classes = num_classes\n",
        "    self.max_seq_length = max_seq_length  # Not used in this specific code, but included for consistency\n",
        "\n",
        "    # Embedding layer\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "    # Self-attention layer\n",
        "    self.self_attention = SelfAttention(embed_dim)\n",
        "\n",
        "    # Linear classifier\n",
        "    self.classifier = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Embedding layer\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # Self-attention layer\n",
        "    x = self.self_attention(x)\n",
        "\n",
        "    # Extract the CLS token (the last token)\n",
        "    cls_token = x[:, -1, :]  # Assuming the last token is the CLS token\n",
        "\n",
        "    # Linear classifier\n",
        "    logits = self.classifier(cls_token)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "OXJzyCq6ulMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyyX_4wB_Xsz"
      },
      "source": [
        "Feel free to change the embedding dimension and observe how it impacts the test accuracy. Note how the model cannot do better than random guessing. Can you think why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLb5yX4GDDdy"
      },
      "outputs": [],
      "source": [
        "kwg = dict(\n",
        "    embed_dim=64,\n",
        "    vocab_size=vocab_size,\n",
        ")\n",
        "train(SimpleTransformer, **kwg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYdD2v3MEXL1"
      },
      "source": [
        "## MLP layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTransformer(nn.Module):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      vocab_size,\n",
        "      mlp_dim=256,\n",
        "      embed_dim=128,\n",
        "      num_classes=2,\n",
        "      max_seq_length=None,\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.mlp_dim = mlp_dim\n",
        "    self.num_classes = num_classes\n",
        "    self.max_seq_length = max_seq_length  # Not used in this specific code, but included for consistency\n",
        "\n",
        "    # Embedding layer\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "    # Self-attention layer\n",
        "    self.self_attention = SelfAttention(embed_dim)\n",
        "\n",
        "    # Linear classifier\n",
        "    self.classifier = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    # MLP layers\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(embed_dim, mlp_dim), nn.ReLU(), nn.Linear(mlp_dim, embed_dim)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    logits = None\n",
        "    # --- YOUR CODE START ---\n",
        "    # x =...  # embedding layer\n",
        "    # x =...  # self-attention layer\n",
        "    # x =...  # mlp layer\n",
        "    # cls_token =...  # extract the CLS token\n",
        "    # logits =...  # linear classifier\n",
        "    # --- YOUR CODE END ---\n",
        "    return logits"
      ],
      "metadata": {
        "id": "uHHUe1dgeWfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQJURdypJK5L"
      },
      "outputs": [],
      "source": [
        "kwg = dict(\n",
        "    embed_dim=64,\n",
        "    mlp_dim=64 * 4,\n",
        "    vocab_size=vocab_size,\n",
        ")\n",
        "train(SimpleTransformer, **kwg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAbACh6agn6H"
      },
      "source": [
        "## positional embeddings\n",
        "One major issue with our model above is that it cannot see the position of the token. For example, the following two examples are similar:\n",
        "- \"123 - 4\"\n",
        "- \"12 - 34\"\n",
        "\n",
        "We will use learned position embeddings below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTransformer(nn.Module):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      vocab_size,\n",
        "      max_seq_length,\n",
        "      mlp_dim=256,\n",
        "      embed_dim=128,\n",
        "      num_classes=2,\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.mlp_dim = mlp_dim\n",
        "    self.num_classes = num_classes\n",
        "    self.max_seq_length = max_seq_length\n",
        "\n",
        "    # Embedding layer\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "    # Positional embedding layer\n",
        "    self.pos_embedding = nn.Embedding(max_seq_length, embed_dim)\n",
        "\n",
        "    # Self-attention layer\n",
        "    self.self_attention = SelfAttention(embed_dim)\n",
        "\n",
        "    # Linear classifier\n",
        "    self.classifier = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    # MLP layers\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(embed_dim, mlp_dim), nn.ReLU(), nn.Linear(mlp_dim, embed_dim)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Embedding layer\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # Positional embeddings\n",
        "    # --- YOUR CODE START ---\n",
        "    # positions =...  # generate position indices\n",
        "    # pos_embeddings =...  # get position embeddings\n",
        "    # x =...  # add scaled position embeddings to input embeddings\n",
        "    # --- YOUR CODE END ---\n",
        "\n",
        "    # Self-attention layer\n",
        "    x = self.self_attention(x)\n",
        "\n",
        "    # MLP layer\n",
        "    x = self.mlp(x)\n",
        "\n",
        "    # Extract the CLS token (the last token)\n",
        "    cls_token = x[:, -1, :]  # Assuming the last token is the CLS token\n",
        "\n",
        "    # Linear classifier\n",
        "    logits = self.classifier(cls_token)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "_Zcw3nDls4iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsLzvQEYDBdM"
      },
      "outputs": [],
      "source": [
        "kwg = dict(\n",
        "    embed_dim=64,\n",
        "    mlp_dim=64 * 4,\n",
        "    vocab_size=vocab_size,\n",
        "    max_seq_length=max_len,\n",
        ")\n",
        "train(SimpleTransformer, **kwg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KCTK3qZKIoU"
      },
      "source": [
        "## normalization layer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTransformer(nn.Module):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      vocab_size,\n",
        "      max_seq_length,\n",
        "      mlp_dim=256,\n",
        "      embed_dim=128,\n",
        "      num_classes=2,\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.mlp_dim = mlp_dim\n",
        "    self.num_classes = num_classes\n",
        "    self.max_seq_length = max_seq_length\n",
        "\n",
        "    # Embedding layer\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "    # Positional embedding layer\n",
        "    self.pos_embedding = nn.Embedding(max_seq_length, embed_dim)\n",
        "\n",
        "    # Self-attention layer\n",
        "    self.self_attention = SelfAttention(embed_dim)\n",
        "\n",
        "    # Linear classifier\n",
        "    self.classifier = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    # MLP layers\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(embed_dim, mlp_dim), nn.ReLU(), nn.Linear(mlp_dim, embed_dim)\n",
        "    )\n",
        "\n",
        "    # Layer Normalization (Specify normalized_shape)\n",
        "    self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
        "    self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Embedding layer\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # Positional embeddings\n",
        "    positions = torch.arange(x.shape[1], device=x.device)\n",
        "    pos_embeddings = self.pos_embedding(positions)\n",
        "    x = x + pos_embeddings / math.sqrt(\n",
        "        self.embed_dim\n",
        "    )  # Scale positional embeddings\n",
        "\n",
        "    # Self-attention layer\n",
        "    x = self.self_attention(x)\n",
        "\n",
        "    # We add layer norm\n",
        "    # --- YOUR CODE START ---\n",
        "    # x =...  # apply layer normalization\n",
        "    # --- YOUR CODE END ---\n",
        "\n",
        "    # MLP layer\n",
        "    x = self.mlp(x)\n",
        "\n",
        "    # We add layer norm\n",
        "    # --- YOUR CODE START ---\n",
        "    # x =...  # apply layer normalization\n",
        "    # --- YOUR CODE END ---\n",
        "\n",
        "    # Extract the CLS token (the last token)\n",
        "    cls_token = x[:, -1, :]  # Assuming the last token is the CLS token\n",
        "\n",
        "    # Linear classifier\n",
        "    logits = self.classifier(cls_token)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "aHAqznY4xcXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6J7822vbK7br"
      },
      "outputs": [],
      "source": [
        "kwg = dict(\n",
        "    embed_dim=64,\n",
        "    mlp_dim=64 * 4,\n",
        "    vocab_size=vocab_size,\n",
        "    max_seq_length=max_len,\n",
        ")\n",
        "train(SimpleTransformer, **kwg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhs0jJUOK7k_"
      },
      "source": [
        "## deeper architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to make the code more concise and readable, we define the following TransformerEncoderBlock."
      ],
      "metadata": {
        "id": "7pXWUT3n00Of"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_dim, mlp_dim):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.mlp_dim = mlp_dim\n",
        "\n",
        "    # Self-attention layer\n",
        "    self.self_attention = SelfAttention(embed_dim)\n",
        "\n",
        "    # Layer normalization\n",
        "    self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    # MLP layers\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(embed_dim, mlp_dim), nn.ReLU(), nn.Linear(mlp_dim, embed_dim)\n",
        "    )\n",
        "\n",
        "    # Layer normalization\n",
        "    self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.layer_norm1(x)\n",
        "    # MLP layer\n",
        "    x = self.mlp(x)\n",
        "    # Layer norm\n",
        "    x = self.layer_norm2(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "_AdS_sK5dp5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTransformer(nn.Module):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      *,\n",
        "      vocab_size,\n",
        "      max_seq_length,\n",
        "      num_layers,\n",
        "      mlp_dim=256,\n",
        "      embed_dim=128,\n",
        "      num_classes=2,\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.mlp_dim = mlp_dim\n",
        "    self.num_classes = num_classes\n",
        "    self.max_seq_length = max_seq_length\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # Embedding layer\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "    # Positional embedding layer\n",
        "    self.pos_embedding = nn.Embedding(max_seq_length, embed_dim)\n",
        "\n",
        "    # Self-attention layer\n",
        "    self.self_attention = SelfAttention(embed_dim)\n",
        "\n",
        "    # Linear classifier\n",
        "    self.classifier = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    # Transformer Encoder Blocks\n",
        "    self.transformer_blocks = nn.ModuleList(\n",
        "        [TransformerEncoderBlock(embed_dim, mlp_dim) for _ in range(num_layers)]\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Embedding layer\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # Positional embeddings\n",
        "    positions = torch.arange(x.shape[1], device=x.device)\n",
        "    pos_embeddings = self.pos_embedding(positions)\n",
        "    x = x + pos_embeddings / math.sqrt(\n",
        "        self.embed_dim\n",
        "    )  # Scale positional embeddings\n",
        "\n",
        "    # Stack multiple transformer encoder blocks\n",
        "    # --- YOUR CODE START ---\n",
        "    # for... in...:  # iterate over transformer blocks\n",
        "    #   x =...  # apply the block\n",
        "    # --- YOUR CODE END ---\n",
        "\n",
        "    # Extract the CLS token (the last token)\n",
        "    cls_token = x[:, -1, :]  # Assuming the last token is the CLS token\n",
        "\n",
        "    # Linear classifier\n",
        "    logits = self.classifier(cls_token)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "NNMprTHPzjKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wek9hVhoL3CO"
      },
      "outputs": [],
      "source": [
        "kwg = dict(\n",
        "    embed_dim=64,\n",
        "    mlp_dim=64 * 4,\n",
        "    vocab_size=vocab_size,\n",
        "    max_seq_length=max_len,\n",
        "    num_layers=3,\n",
        ")\n",
        "train(SimpleTransformer, **kwg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1mCklGoPG4X"
      },
      "source": [
        "## skip connections"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_dim, mlp_dim):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.mlp_dim = mlp_dim\n",
        "\n",
        "    # Self-attention layer\n",
        "    self.self_attention = SelfAttention(embed_dim)\n",
        "\n",
        "    # Layer normalization\n",
        "    self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    # MLP layers\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(embed_dim, mlp_dim), nn.ReLU(), nn.Linear(mlp_dim, embed_dim)\n",
        "    )\n",
        "\n",
        "    # Layer normalization\n",
        "    self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.layer_norm1(x)\n",
        "    y = self.mlp(x)\n",
        "\n",
        "    # We introduce a skip connection\n",
        "    # --- YOUR CODE START ---\n",
        "    #   x =...  # skip connections\n",
        "    # --- YOUR CODE END ---\n",
        "\n",
        "    # Layer norm\n",
        "    x = self.layer_norm2(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "wgwfUMU50_IM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SimpleTransformer is the same, we just update it with the new TransformerEncoderBlock"
      ],
      "metadata": {
        "id": "cQGpJYf61rlC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT_aNKSMJ5u3"
      },
      "outputs": [],
      "source": [
        "class SimpleTransformer(nn.Module):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      *,\n",
        "      vocab_size,\n",
        "      max_seq_length,\n",
        "      num_layers,\n",
        "      mlp_dim=256,\n",
        "      embed_dim=128,\n",
        "      num_classes=2,\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.mlp_dim = mlp_dim\n",
        "    self.num_classes = num_classes\n",
        "    self.max_seq_length = max_seq_length\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # Embedding layer\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "    # Positional embedding layer\n",
        "    self.pos_embedding = nn.Embedding(max_seq_length, embed_dim)\n",
        "\n",
        "    # Self-attention layer\n",
        "    self.self_attention = SelfAttention(embed_dim)\n",
        "\n",
        "    # Linear classifier\n",
        "    self.classifier = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    # Transformer Encoder Blocks\n",
        "    self.transformer_blocks = nn.ModuleList(\n",
        "        [TransformerEncoderBlock(embed_dim, mlp_dim) for _ in range(num_layers)]\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Embedding layer\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # Positional embeddings\n",
        "    positions = torch.arange(x.shape[1], device=x.device)\n",
        "    pos_embeddings = self.pos_embedding(positions)\n",
        "    x = x + pos_embeddings / math.sqrt(\n",
        "        self.embed_dim\n",
        "    )  # Scale positional embeddings\n",
        "\n",
        "    # Stack multiple transformer encoder blocks\n",
        "    for block in self.transformer_blocks:\n",
        "      x = block(x)\n",
        "\n",
        "    # Extract the CLS token (the last token)\n",
        "    cls_token = x[:, -1, :]  # Assuming the last token is the CLS token\n",
        "\n",
        "    # Linear classifier\n",
        "    logits = self.classifier(cls_token)\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvnHNv5QJcY1"
      },
      "outputs": [],
      "source": [
        "kwg = dict(\n",
        "    embed_dim=64,\n",
        "    mlp_dim=64 * 4,\n",
        "    vocab_size=vocab_size,\n",
        "    max_seq_length=max_len,\n",
        "    num_layers=3,\n",
        ")\n",
        "train(SimpleTransformer, **kwg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THm_kx4E9LaY"
      },
      "source": [
        "# Part IV: Transformer Architecture - Sequence generation task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdX3QBfrmOKX"
      },
      "source": [
        "In the previous part, the task was to predict a label.\n",
        "We added a classification head on top of the last output of the transformer sequence to predict it.\n",
        "\n",
        "In this part, we will train a transformer on a sequence generation task.\n",
        "In order to do so, we will apply several key changes :\n",
        "- Cross attention module\n",
        "- Causal masking\n",
        "- Encoder-Decoder architecture\n",
        "- Autoregressive sampling\n",
        "- Perplexity loss instead of the cross-entropy loss\n",
        "\n",
        "We'll go through each point in the following sections :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwWaHq27i3Um"
      },
      "source": [
        "## Dataset - integer addition\n",
        "\n",
        "We'll tackle the problem of **integer addition** using a Transformer model.  Instead of calculating the entire sum at once, we'll train our model to generate the answer **auto-regressively**, predicting one digit at a time, similar to how we learn to do addition manually.\n",
        "\n",
        "We will represent the numbers in a **reversed** manner. For instance, the number 123 will be represented as \"321\".  We'll also pad our input and output sequences with zeros to ensure they have a uniform length and add a special \"#\" character to the output to signal the end of the sum.\n",
        "\n",
        "The following `AdditionTask` class will be responsible for generating our training and eval data, which consists of pairs of reversed addition problems and their corresponding non reversed solutions.\n",
        "\n",
        "Let's dive into the code!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c893FxTHjNR0"
      },
      "outputs": [],
      "source": [
        "# @markdown <font color='blue'>Double click here to see how the dataset is constructed</font>\n",
        "\n",
        "\n",
        "class AdditionTask:\n",
        "  \"\"\"Returns a batch of additions and their results.\n",
        "\n",
        "  This function generates a batch of addition problems where each problem\n",
        "  consists of two randomly generated numbers and their sum. The numbers are\n",
        "  represented as strings in reversed order (e.g., '321' for 123) and are\n",
        "  padded with zeros to ensure uniform length. The result also includes a special\n",
        "  \"#\" character as a separator.\n",
        "\n",
        "  Args:\n",
        "    batch_size: The number of addition problems to generate.\n",
        "    length: The maximum length of the input sequence (including the \"+\" symbol\n",
        "      and padding).\n",
        "\n",
        "  Returns:\n",
        "    A dictionary containing two keys:\n",
        "      - \"input\": A list of strings, where each string is a padded addition\n",
        "      problem in reversed order (e.g., \"123+45\" might be \"321+54000\").\n",
        "      - \"output\": A list of strings, where each string is the result of the\n",
        "      corresponding addition problem, also padded and with a \"#\" (e.g.,\n",
        "      \"664#000\").\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If the provided length is less than or equal to 2.\n",
        "  \"\"\"\n",
        "\n",
        "  def sample_batch(self, batch_size: int, length: int):\n",
        "    \"\"\"Returns a batch of additions and their results.\"\"\"\n",
        "    if length <= 2:\n",
        "      raise ValueError(\"Length must be greater than 2.\")\n",
        "\n",
        "    # We only use `length - 1` tokens for the two values to account for the `+`.\n",
        "    # Generate random lengths for the two numbers in each addition problem.\n",
        "    length_n = np.random.randint(1, length - 1, size=(batch_size,))\n",
        "    length_m = length - 1 - length_n\n",
        "\n",
        "    # Generate random integers based on the calculated lengths.\n",
        "    integer_n = [random.randint(1, 10 ** int(len_n) - 1) for len_n in length_n]\n",
        "    integer_m = [random.randint(1, 10 ** int(len_m) - 1) for len_m in length_m]\n",
        "    # Calculate the sum of the generated integers.\n",
        "    integer_sum = list(map(sum, zip(integer_n, integer_m)))\n",
        "\n",
        "    # Convert integers to reversed strings (e.g., 123 becomes \"321\").\n",
        "    knary_n = [str(x)[::-1] for x in integer_n]\n",
        "    knary_m = [str(x)[::-1] for x in integer_m]\n",
        "\n",
        "    # Create the addition expressions by concatenating the reversed strings with a \"+\".\n",
        "    expressions = [f\"{a}+{b}\" for a, b in zip(knary_n, knary_m)]\n",
        "\n",
        "    # Pad the expressions with zeros to reach the desired length.\n",
        "    expressions = [a + \"\".join([\"0\"] * (length - len(a))) for a in expressions]\n",
        "\n",
        "    # Convert the sums to strings.\n",
        "    results = list(map(str, integer_sum))\n",
        "    # Append \"#\" to the results and pad with zeros.\n",
        "    results = [\n",
        "        res + \"#\" + \"\".join([\"0\"] * (length - len(res))) for res in results\n",
        "    ]\n",
        "    return {\n",
        "        \"input\": expressions,\n",
        "        \"output\": results,\n",
        "    }\n",
        "\n",
        "  @property\n",
        "  def input_size(self) -> int:\n",
        "    \"\"\"Returns the input size for the models.\"\"\"\n",
        "    return 12\n",
        "\n",
        "  @property\n",
        "  def output_size(self) -> int:\n",
        "    \"\"\"Returns the output size for the models.\"\"\"\n",
        "    return 12\n",
        "\n",
        "  @property\n",
        "  def vocab_size(self) -> int:\n",
        "    \"\"\"Returns the output size for the models.\"\"\"\n",
        "    return 12\n",
        "\n",
        "  def output_length(self, input_length: int) -> int:\n",
        "    return input_length + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsZmX5-2kLRX"
      },
      "outputs": [],
      "source": [
        "# # Instantiate an AdditionTask object. This object will handle data generation for our addition task.\n",
        "task = AdditionTask()\n",
        "\n",
        "# Define the maximum length of the addition sequence (including digits, '+', and padding).\n",
        "# We can control the difficulty of the task by changing this value: larger values correspond to more complex addition problems, potentially requiring more steps to solve.\n",
        "# Note that the length must be greater than 2 to accommodate at least one digit for each number plus the '+' symbol.\n",
        "MAX_TRAIN_LENGTH = 10\n",
        "MAX_TEST_LENGTH = 20\n",
        "\n",
        "# Generate a sample batch of addition problems.\n",
        "data = task.sample_batch(batch_size=16, length=MAX_TRAIN_LENGTH)\n",
        "\n",
        "# Let's visualize the generated data using a Pandas DataFrame for better readability.\n",
        "# Each row represents a single addition problem.\n",
        "# The 'input' column shows the addition problem in reversed and padded format (e.g., \"6+54058925\").\n",
        "# The 'output' column shows the corresponding result, not reversed, padded, and with the '#' symbol (e.g., \"52985051#00\").\n",
        "pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The token \"#\" is the EOS (End of Sentence) token. All the 0 characters after it are added with padding, for shape consistency during training and eval.\n",
        "\n",
        "We also fit a tokenizer and check the tokens it assigned"
      ],
      "metadata": {
        "id": "nGZgFFINrVjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and fit the tokenizer\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "    num_words=None,\n",
        "    oov_token=None,\n",
        "    char_level=True,\n",
        ")\n",
        "tokenizer.fit_on_texts(data[\"input\"] + data[\"output\"])\n",
        "tokenizer.word_index"
      ],
      "metadata": {
        "id": "0iOv9oBqvBCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also define a preprocessing function, that we'll be using in the train and eval loops."
      ],
      "metadata": {
        "id": "H9fCyhMArpo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(batch, tokenizer):\n",
        "  \"\"\"Tokenizes and pads the input and output sequences for the model.\n",
        "\n",
        "  Args:\n",
        "    batch: A dictionary containing the input and output sequences as lists of\n",
        "      strings.\n",
        "    tokenizer: A fitted Tokenizer object (e.g., from\n",
        "      torchtext.data.utils.get_tokenizer or a custom implementation).\n",
        "\n",
        "  Returns:\n",
        "    A dictionary containing the processed input and output sequences as PyTorch\n",
        "    tensors,\n",
        "  \"\"\"\n",
        "  # Tokenize the input sequences using the provided tokenizer.\n",
        "  # This converts each string into a sequence of integer indices.\n",
        "  tokens_input = (\n",
        "      torch.tensor(\n",
        "          tokenizer.texts_to_sequences(batch[\"input\"]), dtype=torch.long\n",
        "      )\n",
        "      - 1\n",
        "  )\n",
        "\n",
        "  # Tokenize the output sequences.\n",
        "  tokens_output = (\n",
        "      torch.tensor(\n",
        "          tokenizer.texts_to_sequences(batch[\"output\"]), dtype=torch.long\n",
        "      )\n",
        "      - 1\n",
        "  )\n",
        "\n",
        "  # Pad the sequences to the maximum length within the batch for consistent tensor shapes.\n",
        "  tokens_input = torch.nn.utils.rnn.pad_sequence(tokens_input, batch_first=True)\n",
        "  tokens_output = torch.nn.utils.rnn.pad_sequence(\n",
        "      tokens_output, batch_first=True\n",
        "  )\n",
        "\n",
        "  # Return the processed data as a dictionary.\n",
        "  return dict(input=tokens_input, output=tokens_output)"
      ],
      "metadata": {
        "id": "6wkpJ6Oirv6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_data(data, tokenizer)"
      ],
      "metadata": {
        "id": "NgslMH8Drx3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aciCuNY6BxkZ"
      },
      "source": [
        "## Loss and accuracy functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q7hM17mmJOK"
      },
      "source": [
        "**Connecting to Previous Concepts and Explaining the Loss Function**\n",
        "\n",
        "Recall that in Part III, we dealt with a single-label classification problem.  For that scenario, we used a standard classification loss, as our goal was to predict a single, correct label.\n",
        "\n",
        "In this task, however, we are generating an entire sequence of tokens.  We can think of this as a series of individual classification problems, one for each position in the sequence. Each element in the output sequence requires us to predict the correct token from our vocabulary (digits 0-9, \"+\", and \"#\"). Therefore for each position, the model outputs a probability distribution over all possible tokens, and we need to compare it with the true distribution (one-hot encoded).\n",
        "\n",
        "This is where the `loss_fn` we defined earlier comes in. It computes the cross-entropy loss between the predicted token probabilities and the true one-hot encoded tokens *for each position* in the sequence.  Then, it averages these individual cross-entropy losses across the time dimension (represented by `axis=-1`) to get the overall loss for the sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss definitions"
      ],
      "metadata": {
        "id": "0pupg4i-c7aD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _pointwise_loss_fn(\n",
        "    output: torch.Tensor, target: torch.Tensor\n",
        ") -> torch.Tensor:\n",
        "  \"\"\"Calculates the pointwise cross-entropy loss between predicted probabilities and the true target values.\n",
        "\n",
        "  This function computes the loss for each token in the sequence individually.\n",
        "  \"\"\"\n",
        "  pass\n",
        "  # --- YOUR CODE START ---\n",
        "  # target_one_hot =...  # convert target to one-hot encoding\n",
        "  # return...  # calculate the cross-entropy loss\n",
        "  # --- YOUR CODE END ---\n",
        "\n",
        "\n",
        "# Create the loss and accuracy based on the pointwise ones.\n",
        "def loss_fn(output, target):\n",
        "  \"\"\"Returns the loss between an output and a target, averaged across tokens.\"\"\"\n",
        "  # While in the previous part, we didn't need to average as we predicted a single token.\n",
        "  # We sum over the last dimension (num_classes) and then average\n",
        "  # over all other dimensions (batch_size and sequence_length).\n",
        "  return torch.mean(torch.sum(_pointwise_loss_fn(output, target), dim=-1))"
      ],
      "metadata": {
        "id": "i-uElH4l8Spf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Toy example"
      ],
      "metadata": {
        "id": "YZrXpM4aB7VM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a small toy example to see how the function works."
      ],
      "metadata": {
        "id": "C3atU34bsNvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "sequence_length = 3\n",
        "num_classes = 4  # Let's say we have 4 possible tokens\n",
        "\n",
        "# 1. Dummy Model Output (logits - before softmax):\n",
        "#    Imagine the model predicts the following logits for each token in each sequence:\n",
        "output = torch.tensor(\n",
        "    [\n",
        "        [\n",
        "            [1.0, 2.0, 3.0, 0.5],\n",
        "            [0.1, 0.5, 1.5, 2.0],\n",
        "            [2.5, 1.0, 0.2, 0.1],\n",
        "        ],  # Sequence 1\n",
        "        [\n",
        "            [0.2, 0.3, 0.5, 0.1],\n",
        "            [1.0, 1.5, 2.0, 0.5],\n",
        "            [0.1, 0.2, 0.5, 2.5],\n",
        "        ],  # Sequence 2\n",
        "    ],\n",
        "    dtype=torch.float32,\n",
        ")\n",
        "\n",
        "# 2. Dummy Target :\n",
        "target = torch.Tensor([\n",
        "    [0, 3, 2],\n",
        "    [1, 0, 0],\n",
        "]).long()\n",
        "\n",
        "# 3. We calculate pointwise loss\n",
        "pointwise_loss = _pointwise_loss_fn(output, target)\n",
        "print(\"Pointwise Loss:\\n\", pointwise_loss)\n",
        "\n",
        "# Note that pointwise_loss is a Tensor of shape (batch_size, sequence_length)\n",
        "# We need to average over the sequence dimension losses in this case, as opposed to Part III\n",
        "# where the sequence_length was 1."
      ],
      "metadata": {
        "id": "eOXuzuMvB-j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijOzZIWPnuNe"
      },
      "source": [
        "**Defining Accuracy for Sequence Prediction**\n",
        "\n",
        "Building on our understanding of loss calculation for sequence generation, we now define an accuracy function tailored for this task. As we're predicting a sequence of tokens, we need to consider the correctness of each predicted token within the sequence.\n",
        "\n",
        "The `accuracy_fn` we defined in the cell below does precisely this.  It computes the accuracy for each token position and then averages them. Importantly, it incorporates a masking mechanism (`_accuracy_mask`) to handle sequences of varying lengths. This mask ensures that we only consider the relevant tokens in our accuracy calculation, effectively ignoring any padding tokens that might be present due to the fixed-size nature of our input arrays.\n",
        "\n",
        "In essence, we are evaluating the model's performance on a token-by-token basis, focusing only on the parts of the sequence that correspond to the actual target values. This gives us a more granular and accurate measure of how well the model is learning to generate the correct addition results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSjOhZeBnS3M"
      },
      "outputs": [],
      "source": [
        "def _accuracy_fn(output: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "  \"\"\"Returns the accuracy between an output and a target.\"\"\"\n",
        "  return (torch.argmax(output, dim=-1) == target).float()\n",
        "\n",
        "\n",
        "def accuracy_fn(output, target):\n",
        "  acc = _accuracy_fn(output, target)\n",
        "  return torch.mean(acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPFQ3HRioepb"
      },
      "source": [
        "Similarly to Part III, we define an update function using the losses we just defined."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _apply_loss_and_metrics_fn(\n",
        "    params: dict,\n",
        "    batch: dict[str, torch.Tensor],\n",
        "    model: torch.nn.Module,\n",
        "):\n",
        "  \"\"\"Computes the model output and applies the loss function.\n",
        "\n",
        "  Args:\n",
        "    params: The model parameters (typically the state_dict of a\n",
        "      torch.nn.Module).\n",
        "    batch: The data (consists of both inputs and outputs).\n",
        "    model: The PyTorch model.\n",
        "\n",
        "  Returns:\n",
        "    The loss of the model for the batch of data, extra loss metrics and the\n",
        "    accuracy.\n",
        "  \"\"\"\n",
        "  # Load parameters into the model\n",
        "  model.load_state_dict(params)\n",
        "  model.train()  # Put the model into training mode\n",
        "\n",
        "  outputs = model(inputs=batch[\"input\"], targets=batch[\"output\"], sample=False)\n",
        "\n",
        "  loss = loss_fn(outputs, batch[\"output\"])\n",
        "  accuracy = accuracy_fn(outputs, batch[\"output\"])\n",
        "  return loss, (accuracy)\n",
        "\n",
        "\n",
        "def _update_parameters(\n",
        "    params: dict,\n",
        "    batch: dict[str, torch.Tensor],\n",
        "    model: torch.nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    opt_state: Any,\n",
        ") -> tuple[Any, Any, tuple[float, float]]:\n",
        "  \"\"\"Applies a single SGD update step to the model parameters.\n",
        "\n",
        "  Args:\n",
        "    params: The model parameters (typically the state_dict of a\n",
        "      torch.nn.Module).\n",
        "    batch: The data (consists of both inputs and outputs).\n",
        "    model: The PyTorch model.\n",
        "    optimizer: The optimizer that computes the updates from the gradients of the\n",
        "      `loss_fn` with respect to the `params` and the previous `opt_state`.\n",
        "    opt_state: The optimizer state, e.g., momentum for each variable when using\n",
        "      Adam. Not used in this case, but kept for consistency with original code.\n",
        "\n",
        "  Returns:\n",
        "    The updated parameters, the new optimizer state, and the loss, loss metrics\n",
        "    and accuracy.\n",
        "  \"\"\"\n",
        "\n",
        "  # Reset gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # Compute loss and gradients\n",
        "  loss, (accuracy) = _apply_loss_and_metrics_fn(params, batch, model)\n",
        "  loss.backward()\n",
        "\n",
        "  # Update parameters\n",
        "  optimizer.step()\n",
        "\n",
        "  # Get updated model parameters\n",
        "  new_params = model.state_dict()\n",
        "\n",
        "  # Update optimizer state (not strictly necessary for optimizers like Adam, but a good practice)\n",
        "  # Since opt_state is not used here, we can just pass it back\n",
        "  new_opt_state = optimizer.state_dict()\n",
        "\n",
        "  return new_params, new_opt_state, (loss.item(), accuracy.item())"
      ],
      "metadata": {
        "id": "XOmcwYUz2NnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSuwfAmKrLdT"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2nPOBamrOD6"
      },
      "source": [
        "In order to focus on the architectural changes in the next part, we define a standard training loop, which we will be using across the cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkmrKhtKrdwA"
      },
      "outputs": [],
      "source": [
        "# @markdown <font color=\"blue\">Double click here to see the full training loop</font>\n",
        "def run_training(\n",
        "    *,\n",
        "    task: Any,\n",
        "    model: torch.nn.Module,\n",
        "    max_sequence_length: int,\n",
        "    train_steps: int = 10_000,\n",
        "    seed: int = 0,  # Used to sample during forward pass (e.g. from final logits).\n",
        "    model_init_seed: int = 0,  # Used to initialize model parameters.\n",
        "    log_frequency: int = 50,\n",
        "    batch_size: int = 128,\n",
        "    learning_rate: float = 1e-3,\n",
        "    max_grad_norm: float = 1.0,\n",
        ") -> Tuple[pd.DataFrame, dict, Any]:\n",
        "  \"\"\"Trains the model with the provided config.\"\"\"\n",
        "\n",
        "  # Fix the seeds\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  # torch.manual_seed(seed)\n",
        "  # torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  # Use GPU if available\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.to(device)\n",
        "\n",
        "  # Define the optimizer\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  # Sample a batch to fit the tokenizer\n",
        "  dummy_batch = task.sample_batch(\n",
        "      length=max_sequence_length,\n",
        "      batch_size=256,\n",
        "  )\n",
        "\n",
        "  # Define and fit the tokenizer\n",
        "  tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      num_words=None,\n",
        "      oov_token=None,\n",
        "      char_level=True,\n",
        "  )\n",
        "  tokenizer.fit_on_texts(dummy_batch[\"input\"] + dummy_batch[\"output\"])\n",
        "\n",
        "  print(f\"The tokenizer index is: {tokenizer.word_index}\")\n",
        "\n",
        "  params = model.state_dict()\n",
        "  opt_state = optimizer.state_dict()\n",
        "\n",
        "  results = []\n",
        "  for step in tqdm.tqdm(range(train_steps + 1)):\n",
        "    # Randomness handled by either python.random or numpy.\n",
        "    length = random.choice(list(range(3, max_sequence_length + 1)))\n",
        "\n",
        "    # Randomness handled by either torch, python.random or numpy.\n",
        "    train_batch = task.sample_batch(length=length, batch_size=batch_size)\n",
        "    train_batch = preprocess_data(train_batch, tokenizer)\n",
        "\n",
        "    # Move batch to device\n",
        "    train_batch[\"input\"] = train_batch[\"input\"].to(device)\n",
        "    train_batch[\"output\"] = train_batch[\"output\"].to(device)\n",
        "\n",
        "    # Update the parameters.\n",
        "    params, opt_state, (train_loss, train_accuracy) = _update_parameters(\n",
        "        params=params,\n",
        "        batch=train_batch,\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        opt_state=opt_state,\n",
        "    )\n",
        "\n",
        "    # Log the training metrics\n",
        "    if (log_frequency > 0) and (step % log_frequency == 0):\n",
        "      log_data = {\n",
        "          \"step\": step,\n",
        "          \"train_loss\": float(train_loss),\n",
        "          \"train_accuracy\": float(train_accuracy),\n",
        "      }\n",
        "      print(log_data)\n",
        "      results.append(log_data)\n",
        "\n",
        "  df_results = pd.DataFrame(results)\n",
        "  return df_results, params, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbPdgfV_KhmE"
      },
      "source": [
        "## Eval function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmYsdIdGe3TG"
      },
      "source": [
        "Next, we define the eval loop function. We'll be running this function at the end of each model change, it will be unchanged across the rest of the colab."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown <font color=\"blue\">Double click here to see the full eval code</font>\n",
        "\n",
        "\n",
        "def run_evaluation(\n",
        "    *,\n",
        "    model: torch.nn.Module,\n",
        "    params: Any,\n",
        "    tokenizer: Any,\n",
        "    task: Any,\n",
        "    max_test_length: int = 20,  # The largest sequence length to evaluate on\n",
        "    total_batch_size: int = 512,\n",
        "    sub_batch_size: int = 64,  # We use this to avoid memory overflow.\n",
        "    seed: int = 1,\n",
        "    is_autoregressive: bool = False,\n",
        ") -> pd.DataFrame:\n",
        "  \"\"\"Evaluates the model on addition problems of various lengths and logs the results.\n",
        "\n",
        "  This function tests the model's ability to generalize to sequences longer than\n",
        "  those seen during training.\n",
        "  It generates batches of addition problems with increasing lengths, evaluates\n",
        "  the model's accuracy on each batch,\n",
        "  and returns a Pandas DataFrame containing the accuracies for each length.\n",
        "\n",
        "  Args:\n",
        "    model: The PyTorch model to evaluate.\n",
        "    params: The trained model parameters (a state_dict).\n",
        "    tokenizer: The Tokenizer used to convert between text and token IDs.\n",
        "    task: An instance of the AdditionTask class, used for generating data.\n",
        "    max_test_length: The maximum length of sequences to evaluate on.\n",
        "    total_batch_size: The total number of examples to evaluate for each length.\n",
        "    sub_batch_size: The size of each sub-batch used during evaluation (to avoid\n",
        "      memory issues).\n",
        "    seed: The random seed for reproducibility.\n",
        "    is_autoregressive: A boolean indicating whether the model is autoregressive\n",
        "      (if True, it expects an additional 'sample' argument during inference).\n",
        "\n",
        "  Returns:\n",
        "    A pandas DataFrame with columns 'length' and 'accuracy', where each row\n",
        "    represents the model's accuracy on sequences of a given length.\n",
        "  \"\"\"\n",
        "\n",
        "  # Fix the random seed for reproducibility.\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  # torch.manual_seed(seed)\n",
        "  # torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  # Use GPU if available\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  # Load parameters into the model\n",
        "  model.load_state_dict(params)\n",
        "  model.to(device)\n",
        "  model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "  results = []\n",
        "  lengths = range(3, max_test_length + 1)\n",
        "\n",
        "  # Iterate over different sequence lengths\n",
        "  for length in tqdm.tqdm(lengths, desc=\"Lengths\"):\n",
        "    sub_accuracies = []\n",
        "\n",
        "    # Evaluate on multiple sub-batches to avoid memory overflow.\n",
        "    for _ in range(total_batch_size // sub_batch_size):\n",
        "      # Generate a batch of addition problems with the current length.\n",
        "      batch = task.sample_batch(sub_batch_size, length)\n",
        "      batch = preprocess_data(batch, tokenizer)\n",
        "\n",
        "      # Move batch to device\n",
        "      batch[\"input\"] = batch[\"input\"].to(device)\n",
        "      batch[\"output\"] = batch[\"output\"].to(device)\n",
        "\n",
        "      # Run the model to get predictions.\n",
        "      with torch.no_grad():\n",
        "        outputs = model(\n",
        "            inputs=batch[\"input\"],\n",
        "            targets=batch[\"output\"],\n",
        "            sample=is_autoregressive,\n",
        "        )\n",
        "\n",
        "        # Calculate the accuracy for the current sub-batch.\n",
        "        sub_accuracies.append(\n",
        "            float(accuracy_fn(outputs, batch[\"output\"]).cpu().numpy())\n",
        "        )\n",
        "\n",
        "    # Calculate the average accuracy for the current length.\n",
        "    log_data = {\n",
        "        \"length\": length,\n",
        "        \"accuracy\": np.mean(sub_accuracies),\n",
        "    }\n",
        "    print(log_data)\n",
        "    results.append(log_data)\n",
        "\n",
        "  # Return the results as a pandas DataFrame.\n",
        "  return pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "EPs2IQmZFN57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15ZxHjEZW0BY"
      },
      "source": [
        "## Architecture - Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpP_IqP8LejQ"
      },
      "source": [
        "In Part III, we explored an encoder-only Transformer architecture. Recall that such a configuration is particularly well-suited for tasks like classification, where the primary goal is to map an input sequence to a single output label or category.\n",
        "\n",
        "The implementation below is slightly different than the one above, but the core elements are identical."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We redefine the Transformer Encoder module, similarly to Part III,\n",
        "with the following differences:\n",
        "- We don't add positional embeddings immmediately\n",
        "- We remove the output classifier and return the full sequence of embeddings."
      ],
      "metadata": {
        "id": "yTP4Qqw_G7YT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseTransformerEncoder(nn.Module):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      *,\n",
        "      vocab_size,\n",
        "      max_seq_length,\n",
        "      num_layers,\n",
        "      mlp_dim=256,\n",
        "      embed_dim=128,\n",
        "      num_classes=2,\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.mlp_dim = mlp_dim\n",
        "    self.num_classes = num_classes\n",
        "    self.max_seq_length = max_seq_length\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # Embedding layer\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "    # Positional embedding layer\n",
        "    self.pos_embedding = nn.Embedding(max_seq_length, embed_dim)\n",
        "\n",
        "    # Self-attention layer\n",
        "    self.self_attention = SelfAttention(embed_dim)\n",
        "\n",
        "    # Transformer Encoder Blocks\n",
        "    self.transformer_blocks = nn.ModuleList(\n",
        "        [TransformerEncoderBlock(embed_dim, mlp_dim) for _ in range(num_layers)]\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Embedding layer\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # Positional embeddings\n",
        "    positions = torch.arange(x.shape[1], device=x.device)\n",
        "    pos_embeddings = self.pos_embedding(positions)\n",
        "    x = x + pos_embeddings / math.sqrt(\n",
        "        self.embed_dim\n",
        "    )  # Scale positional embeddings\n",
        "\n",
        "    # Stack multiple transformer encoder blocks\n",
        "    for block in self.transformer_blocks:\n",
        "      x = block(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Z3XvPPm3cMRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQX_mwRzjgAe"
      },
      "source": [
        "## Architecture - Cross attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSmPXPfRjlR_"
      },
      "source": [
        "**Introducing Cross-Attention: Bridging the Gap Between Encoder and Decoder**\n",
        "\n",
        "In the previous sections, we explored the self-attention mechanism, a powerful tool that allows a model to weigh the importance of different parts of an input sequence when processing it.  Self-attention operates within a single sequence, where the query, key, and value vectors are all derived from the same input.\n",
        "\n",
        "Now, we introduce a generalization of this concept: **cross-attention**. While conceptually similar to self-attention, cross-attention enables the model to attend to information from a *different* source sequence. This opens up exciting possibilities for tasks that require relating information between two distinct sequences, such as the sequence-to-sequence task that the transformer was originally designed for.\n",
        "\n",
        "In the context of the Transformer architecture, cross-attention serves as the crucial bridge between the encoder and the decoder. The decoder uses cross-attention to focus on relevant parts of the encoded input sequence (produced by the encoder) while generating the output sequence. This allows the model to effectively translate, summarize, or otherwise transform the input based on the learned relationships between the two sequences. In the following code, we implement a `CrossAttention` class. The main difference with self-attention is that the keys and values will be sourced from a different source than the query."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the self-attention layer\n",
        "class CrossAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_dim):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "\n",
        "    # Calculate query, key, and value matrices using linear layers\n",
        "    # --- YOUR CODE START ---\n",
        "    # self.query = ...\n",
        "    # self.key = ...\n",
        "    # self.value = ...\n",
        "    # --- YOUR CODE END ---\n",
        "\n",
        "  def forward(\n",
        "      self, inputs_q: torch.Tensor, inputs_kv: torch.Tensor\n",
        "  ) -> torch.Tensor:\n",
        "    # Calculate query, key, and value matrices\n",
        "\n",
        "    # --- YOUR CODE START ---\n",
        "    # query = self.query(...)\n",
        "    # key = self.key(...)\n",
        "    # value = self.value(...)\n",
        "\n",
        "    # Calculate attention scores (scaled dot-product attention)\n",
        "\n",
        "    # attention_scores = torch.matmul(..., ...) / math.sqrt(self.embed_dim)\n",
        "    # attention_weights = nn.functional.softmax(attention_scores, dim=...)\n",
        "\n",
        "    # Apply attention weights to values\n",
        "    # output = torch.matmul(..., ...)\n",
        "    # --- YOUR CODE END ---\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "HiWbvkjPVE7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6pQi-rlgbHd"
      },
      "source": [
        "## Architecture - Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Motivation"
      ],
      "metadata": {
        "id": "0apKuDincf80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introducing the Decoder: Generating Output Sequences**\n",
        "\n",
        "In Part III, our focus was on classification, where the model's objective was to predict a single label for a given input sequence.  In this section, we'll be tackling a more complex task: **sequence generation**. This requires us to introduce the **Decoder** component of the Transformer architecture.\n",
        "\n",
        "The Decoder's role is to take the encoded representation of the input sequence (generated by the Encoder) and generate an output sequence, one token at a time. While its architecture shares many similarities with the Encoder, there are some crucial distinctions that enable it to perform this generative task:\n",
        "\n",
        "**Key Differences Between the Decoder and Encoder:**\n",
        "\n",
        "1.  **Cross-Attention:**  Perhaps the most significant difference is the inclusion of **cross-attention** layers. In each decoder layer, cross-attention allows the decoder to attend to the *encoder's* output. This enables the decoder to incorporate information from the input sequence when generating the output. The decoder's embeddings are used as queries, while the encoder's embeddings are used as keys and values.\n",
        "2.  **Shifted Right Input:** During training, the decoder's input is the target sequence, *shifted one position to the right* and prepended with a special \"start\" token. This is done to facilitate **teacher forcing**, where the model is trained to predict the next token given the *previous* ground-truth tokens. The reason is that, at inference time, the model will generate the sequence auto-regressively, therefore it is useful to train it with a similar distribution of inputs.\n",
        "3.  **Causal Masking:** To prevent the decoder from \"cheating\" during training and peeking at future tokens, a **causal mask** (also known as a look-ahead mask) is applied during self-attention within the decoder. This mask ensures that the model can only attend to tokens that have already been generated, effectively simulating the auto-regressive nature of sequence generation during inference.\n",
        "\n",
        "**Detailed Exploration in the Following Cells:**\n",
        "\n",
        "We'll delve into each of these key features  cross-attention, shifted right input, and causal masking  in more detail in the following cells. We'll examine their implementation and understand their crucial role in enabling the decoder to effectively generate sequences. By understanding these components, we'll gain a deeper appreciation for the Transformer's ability to tackle complex sequence-to-sequence tasks."
      ],
      "metadata": {
        "id": "EDNlHsd6O32f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shift the decoder input to the right"
      ],
      "metadata": {
        "id": "QpuMeXGj00Aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def shift_right(x: torch.Tensor, vocab_size: int) -> torch.Tensor:\n",
        "  \"\"\"Shifts the target sequence one step to the right and pads with a special start token.\n",
        "\n",
        "  This function prepares the target sequence for teacher forcing during\n",
        "  training.\n",
        "  By shifting the target sequence to the right and adding a special start token\n",
        "  at the beginning,\n",
        "  we provide the model with the previous token as input when predicting the\n",
        "  current token.\n",
        "\n",
        "  Returns:\n",
        "    The shifted and padded target sequence, one-hot encoded. Expected shape:\n",
        "    (batch_size, sequence_length)\n",
        "  \"\"\"\n",
        "  # Add a time dimension for the single-output case (i.e., when x is a single sequence instead of a batch).\n",
        "  if x.ndim == 1:\n",
        "    x = x.unsqueeze(1)\n",
        "\n",
        "  # Pad the sequence at the beginning with the special start token (represented by output_size).\n",
        "  # Remove the last element to maintain the original sequence length.\n",
        "  # The sequence is now shifted to the right, and the first token is the start token.\n",
        "\n",
        "  # --- YOUR CODE START ---\n",
        "  # return F.pad(..., (1, 0), mode='constant', value=...)\n",
        "  # --- YOUR CODE END ---"
      ],
      "metadata": {
        "id": "FaBJvC1VXH4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's walk through the code step by step"
      ],
      "metadata": {
        "id": "YjI9bhj_1TId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the output size (vocabulary size).\n",
        "output_size = 4  # Tokens will be 0, 1, 2, 3\n",
        "\n",
        "# Create a one-hot encoded toy input sequence.\n",
        "# Represents the sequence [2, 3, 0]\n",
        "x = torch.tensor(\n",
        "    [[\n",
        "        [0, 0, 1, 0],  # Token 2\n",
        "        [0, 0, 0, 1],  # Token 3\n",
        "        [1, 0, 0, 0],  # Token 0\n",
        "    ]],\n",
        "    dtype=torch.float32,\n",
        ")\n",
        "print(f\"input : \\n{x}\\n\")\n",
        "\n",
        "# Compute the argmax\n",
        "x = torch.argmax(x, dim=-1)\n",
        "print(f\"argmax : {x}\\n\")\n",
        "\n",
        "padded = F.pad(x[:, :-1], (1, 0), mode=\"constant\", value=output_size)\n",
        "print(f\"padded : \\n{padded}\\n\")"
      ],
      "metadata": {
        "id": "PST7kfm_1Yn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder architecture"
      ],
      "metadata": {
        "id": "t-esEGLt2pWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_dim, mlp_dim):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.mlp_dim = mlp_dim\n",
        "\n",
        "    # Self-attention layer\n",
        "    self.self_attention = SelfAttention(embed_dim)\n",
        "    self.cross_attention = CrossAttention(embed_dim)\n",
        "\n",
        "    # Layer normalization\n",
        "    self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    # MLP layers\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(embed_dim, mlp_dim), nn.ReLU(), nn.Linear(mlp_dim, embed_dim)\n",
        "    )\n",
        "\n",
        "    # Layer normalization\n",
        "    self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "  def forward(self, enc_emb, dec_emb):\n",
        "    # --- YOUR CODE START ---\n",
        "    # x = self.self_attention(...)\n",
        "    # x = self.layer_norm1(x)\n",
        "\n",
        "    # y = self.cross_attention(..., ...)\n",
        "    y = self.layer_norm1(y)\n",
        "\n",
        "    # MLP layer\n",
        "    y = self.mlp(y)\n",
        "    # Layer norm\n",
        "    y = self.layer_norm2(y)\n",
        "\n",
        "    # --- YOUR CODE END ---\n",
        "    return y"
      ],
      "metadata": {
        "id": "dFEsIcMPYlws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseTransformerDecoder(nn.Module):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      *,\n",
        "      vocab_size,\n",
        "      max_seq_length,\n",
        "      num_layers,\n",
        "      mlp_dim=256,\n",
        "      embed_dim=128,\n",
        "      num_classes=2,\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.mlp_dim = mlp_dim\n",
        "    self.num_classes = num_classes\n",
        "    self.max_seq_length = max_seq_length\n",
        "    self.num_layers = num_layers\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "    # Embedding layer\n",
        "    self.embedding = nn.Embedding(vocab_size + 1, embed_dim)\n",
        "\n",
        "    # Positional embedding layer\n",
        "    self.pos_embedding = nn.Embedding(max_seq_length, embed_dim)\n",
        "\n",
        "    # Self-attention layer\n",
        "    self.self_attention = SelfAttention(embed_dim)\n",
        "\n",
        "    # Transformer Encoder Blocks\n",
        "    # --- YOUR CODE START ---\n",
        "    # self.transformer_blocks = nn.ModuleList(\n",
        "    # [TransformerDecoderBlock(..., ...) for _ in range(...)]\n",
        "    # )\n",
        "    # --- YOUR CODE END ---\n",
        "\n",
        "  def forward(\n",
        "      self, encoded: torch.Tensor, targets: torch.Tensor\n",
        "  ) -> torch.Tensor:\n",
        "    # Prepare the target sequence for teacher forcing by shifting it to the right.\n",
        "    x = shift_right(targets, self.vocab_size)\n",
        "\n",
        "    # Embedding layer\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # Positional embeddings\n",
        "    positions = torch.arange(x.shape[1], device=x.device)\n",
        "    pos_embeddings = self.pos_embedding(positions)\n",
        "    x = x + pos_embeddings / math.sqrt(\n",
        "        self.embed_dim\n",
        "    )  # Scale positional embeddings\n",
        "\n",
        "    # Stack multiple transformer encoder blocks\n",
        "    # --- YOUR CODE START ---\n",
        "    # for block in self.transformer_blocks:\n",
        "    # x = block(..., ...)\n",
        "    # --- YOUR CODE END ---\n",
        "    return x"
      ],
      "metadata": {
        "id": "G-smF6CxJFHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJuRSC2bzlXX"
      },
      "source": [
        "## Architecture Encoder-Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Combining the encoder and decoder"
      ],
      "metadata": {
        "id": "o6O61faR32ca"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ3tfMMhqxKS"
      },
      "source": [
        "**Connecting Encoder and Decoder: Passing Information for Sequence Generation**\n",
        "\n",
        "The core strength of the Transformer architecture lies in its ability to effectively process and generate sequences.  A crucial element in this process is the seamless flow of information from the encoder to the decoder.  As we've seen, the encoder's role is to create a rich, contextualized representation of the input sequence.  Now, it's the decoder's job to take that representation and generate the corresponding output sequence.\n",
        "\n",
        "To achieve this, we **pass the encoder's output as an input to the decoder**. This encoded representation serves as the decoder's \"memory\" of the input sequence.  By using cross-attention layers, the decoder can then selectively attend to different parts of this encoded input while generating each token of the output sequence. This allows the decoder to make informed decisions based on the entire context of the input, leading to more accurate and coherent output generation. The `targets` will be used by the decoder to guide its generation process, computing a loss with the prediction at each step."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseTransformer(nn.Module):\n",
        "  \"\"\"Transformer model for sequence-to-sequence tasks.\n",
        "\n",
        "  This class combines the encoder and decoder modules to create a complete\n",
        "  Transformer model.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      num_layers: int,\n",
        "      max_seq_length: int,\n",
        "      vocab_size: int,\n",
        "      embed_dim: int = 32,\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.num_layers = num_layers\n",
        "    self.embed_dim = embed_dim\n",
        "    self.max_seq_length = max_seq_length\n",
        "\n",
        "    # Create an instance of the TransformerEncoder.\n",
        "    # --- YOUR CODE START ---\n",
        "\n",
        "    # self.encoder = ...\n",
        "\n",
        "    # Create an instance of the TransformerDecoder.\n",
        "    # self.decoder = ...\n",
        "\n",
        "    # Create a dense layer to project the decoder output to the vocabulary space.\n",
        "    # self.linear_output = ...\n",
        "\n",
        "    # --- YOUR CODE END ---\n",
        "\n",
        "  def forward(\n",
        "      self, inputs: torch.Tensor, targets: torch.Tensor, sample: bool\n",
        "  ) -> torch.Tensor:\n",
        "    \"\"\"Applies the Transformer model to the given input and target sequences.\"\"\"\n",
        "\n",
        "    del sample  # For now it's a dummy variable, we'll use it later.\n",
        "    logits = None\n",
        "    # --- YOUR CODE START ---\n",
        "\n",
        "    # Encode the input sequence.\n",
        "    # encoder_output = ...\n",
        "\n",
        "    # Decode the encoded input, using the target sequence for teacher forcing.\n",
        "    # decoder_output = ...\n",
        "\n",
        "    # Project the decoder output to the vocabulary space to get logits.\n",
        "    # logits = ...\n",
        "\n",
        "    # --- YOUR CODE END ---\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "IYpwYRWUaFwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ],
      "metadata": {
        "id": "9Ews5ia_37x-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vs-KO4uazqzB"
      },
      "source": [
        "Let's train the model above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ud3ROjnNV2E-"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "model = BaseTransformer(\n",
        "    num_layers=1,\n",
        "    vocab_size=task.vocab_size,\n",
        "    max_seq_length=MAX_TEST_LENGTH + 2,\n",
        ")\n",
        "\n",
        "# Run the training loop\n",
        "df_train, params, tokenizer = run_training(\n",
        "    max_sequence_length=MAX_TRAIN_LENGTH,\n",
        "    task=task,\n",
        "    model=model,\n",
        "    batch_size=128,\n",
        "    train_steps=2_500,\n",
        ")\n",
        "\n",
        "# Plot the training accuracy over training\n",
        "sns.lineplot(data=df_train, x=\"step\", y=\"train_accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Eval"
      ],
      "metadata": {
        "id": "ZduF98It3_v2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzE0OYKl0_3N"
      },
      "source": [
        "Let's evaluate the model at test time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKh9fBlM1H74"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "df_eval = run_evaluation(\n",
        "    model=model,\n",
        "    params=params,\n",
        "    tokenizer=tokenizer,\n",
        "    task=task,\n",
        "    max_test_length=MAX_TEST_LENGTH,\n",
        ")\n",
        "\n",
        "# Save the eval data for later comparisons across experiments\n",
        "df_eval_dict = dict(base=df_eval)\n",
        "\n",
        "# Plot the test accuracy for each length\n",
        "sns.lineplot(data=df_eval, x=\"length\", y=\"accuracy\", marker=\"o\").set_ylim(\n",
        "    -0.05, 1.05\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Analysis"
      ],
      "metadata": {
        "id": "d8ha4AED4Kbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A reminder that the lengths [3, 10] are in distribution wrt the training dataset, while [11, 20] are out of distribution. Also, note that the in distribution eval dataset may be contaminated with some training samples.\n",
        "\n",
        "- We note that the average accuracy is better than random (<10% accuracy) for all lengths.\n",
        "- We also note that the test accuracy decays with the sequence length, it can be explained by being more out of distribution as well as the increase of the task complexity."
      ],
      "metadata": {
        "id": "Gb6Qxz9o4Mmf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmU-xCPYB21T"
      },
      "source": [
        "## Autoregressive sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Motivation"
      ],
      "metadata": {
        "id": "p-rmajibZfv7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3k75UwBz8P_"
      },
      "source": [
        "**Autoregressive Sampling: Generating Sequences Token by Token**\n",
        "\n",
        "Now that we have a trained autoregressive Transformer, we can use it to generate new sequences.  Unlike training, where we had access to the entire target sequence and could use teacher forcing, at inference time, we need to generate the output sequence **one token at a time**, using the model's own predictions as input for the subsequent tokens. This process is called **autoregressive sampling**.\n",
        "\n",
        "During autoregressive sampling, the model starts with an initial input (usually just the input sequence for the encoder) and a special \"start\" token for the decoder. It then predicts the probability distribution over the output vocabulary for the first token. We sample a token from this distribution, append it to the decoder's input, and repeat the process. This continues until a special \"end\" token is generated or a maximum sequence length is reached. Each prediction is conditioned on the input sequence and the previously generated tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Code : Add greedy autoregressive sampling"
      ],
      "metadata": {
        "id": "_A2Z61i6XXBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_transformer_autoregressive(TransformerModel):\n",
        "  \"\"\"Wraps a Transformer model to make it autoregressive.\n",
        "\n",
        "  This function modifies a standard Transformer model to generate outputs\n",
        "  autoregressively,\n",
        "  one token at a time.\n",
        "  \"\"\"\n",
        "\n",
        "  class AutoregressiveTransformer(nn.Module):\n",
        "    \"\"\"Autoregressive Transformer model for sequence generation.\n",
        "\n",
        "    This class wraps a standard Transformer model to enable autoregressive\n",
        "    generation.\n",
        "    In autoregressive mode, the model generates the output sequence one token at\n",
        "    a time,\n",
        "    using its previous predictions as input for subsequent tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_layers: int, max_seq_length: int, vocab_size: int):\n",
        "      super().__init__()\n",
        "      self.vocab_size = vocab_size\n",
        "      self.num_layers = num_layers\n",
        "      self.max_seq_length = max_seq_length\n",
        "      self.model = TransformerModel(\n",
        "          vocab_size=vocab_size,\n",
        "          num_layers=num_layers,\n",
        "          max_seq_length=max_seq_length,\n",
        "      )\n",
        "\n",
        "    def forward(\n",
        "        self, inputs: torch.Tensor, targets: torch.Tensor, sample: bool\n",
        "    ) -> torch.Tensor:\n",
        "      \"\"\"Applies the autoregressive Transformer model.\"\"\"\n",
        "      # Determine the output length based on the targets shape. output_length will be target_sequence_length, or 1 in the case of a single prediction.\n",
        "      output_length = 1 if len(targets.shape) == 2 else targets.shape[1]\n",
        "      # Get the output size from the targets.\n",
        "      output_size = targets.shape[-1]\n",
        "\n",
        "      if not sample or output_length == 1:\n",
        "        output = self.model(inputs, targets, sample)\n",
        "      else:\n",
        "\n",
        "        def evaluate_ model_autoregressively(idx, predictions):\n",
        "          \"\"\"Iteratively evaluates the model based on the previous predictions.\n",
        "\n",
        "          This function performs a single step of autoregressive generation.\n",
        "          It takes the current predictions, predicts the next token, and updates\n",
        "          the predictions array.\n",
        "\n",
        "          Args:\n",
        "              idx: The index of the target sequence that should be evaluated.\n",
        "              predictions: The logits for the predictions up to but not\n",
        "                including the index `idx`.\n",
        "\n",
        "          Returns:\n",
        "              The `predictions` array modified only at position `idx` where the\n",
        "              logits for index `idx` have been inserted.\n",
        "          \"\"\"\n",
        "\n",
        "          # --- YOUR CODE START ---\n",
        "\n",
        "          # Apply the model to get the logits for the next token.\n",
        "          logits = self.model(inputs, ..., sample)\n",
        "\n",
        "          # Update the predictions array with the new logits at the current index.\n",
        "          # return predictions.index_put(\n",
        "          # indices=...,\n",
        "          # values=logits[..., ...]\n",
        "          # )\n",
        "\n",
        "          # --- YOUR CODE END ---\n",
        "\n",
        "        # Use torch.for_loop for efficient autoregressive generation.\n",
        "        output = torch.for_loop(\n",
        "            lower=0,\n",
        "            upper=output_length,\n",
        "            body_fun=evaluate_model_autoregressively,\n",
        "            init_val=torch.empty_like(targets),\n",
        "        )\n",
        "\n",
        "      return output\n",
        "\n",
        "  return AutoregressiveTransformer"
      ],
      "metadata": {
        "id": "FI2LMvBla_hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ],
      "metadata": {
        "id": "j2p4BMirY6fJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d65Zu_WeDOc"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "AutoregressiveTransformer = make_transformer_autoregressive(BaseTransformer)\n",
        "model = AutoregressiveTransformer(\n",
        "    num_layers=1,\n",
        "    vocab_size=task.vocab_size,\n",
        "    max_seq_length=MAX_TEST_LENGTH + 2,\n",
        ")\n",
        "\n",
        "# Run the training loop.\n",
        "df_train, params, tokenizer = run_training(\n",
        "    max_sequence_length=MAX_TRAIN_LENGTH,\n",
        "    task=task,\n",
        "    model=model,\n",
        "    batch_size=128,\n",
        "    train_steps=2_500,\n",
        ")\n",
        "\n",
        "# Visualize the training accuracy.\n",
        "sns.lineplot(data=df_train, x=\"step\", y=\"train_accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Eval"
      ],
      "metadata": {
        "id": "_ZiwCtEwY9KZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qON8GsXKjEzz"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "\n",
        "df_eval = run_evaluation(\n",
        "    model=model,\n",
        "    params=params,\n",
        "    tokenizer=tokenizer,\n",
        "    task=task,\n",
        "    max_test_length=MAX_TEST_LENGTH,\n",
        "    is_autoregressive=True,\n",
        ")\n",
        "# Save the eval data for later comparisons across experiments\n",
        "df_eval_dict[\"base_autoregressive\"] = df_eval\n",
        "\n",
        "# Plot the test accuracy for each length\n",
        "sns.lineplot(data=df_eval, x=\"length\", y=\"accuracy\", marker=\"o\").set_ylim(\n",
        "    -0.05, 1.05\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Analysis"
      ],
      "metadata": {
        "id": "SNz6q7MnZACQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_eval_all = pd.concat(\n",
        "    df_eval_dict, keys=df_eval_dict.keys(), names=[\"experiment\", None]\n",
        ")\n",
        "sns.lineplot(\n",
        "    data=df_eval_all, x=\"length\", y=\"accuracy\", hue=\"experiment\", marker=\"o\"\n",
        ").set_ylim(-0.05, 1.05)"
      ],
      "metadata": {
        "id": "Cb1IYOEM4M-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We note that the autoregressive model has a lower accuracy than the non autoregressive model. The reason is that the non autoregressive model has accress to the ground truth sequence as an input, while the autoregressive model doesn't. Because it only sees the past generated tokens."
      ],
      "metadata": {
        "id": "BwlNL_pP7Ooe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y394g_GdCjCm"
      },
      "source": [
        "## Add causal masks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Motivation"
      ],
      "metadata": {
        "id": "0Z2YgxhXZqYj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QYaSlqV1hrx"
      },
      "source": [
        "**Causal Masking: Enforcing Autoregressive Generation in the Decoder**\n",
        "\n",
        "In the decoder, we need to ensure that the model generates the output sequence **autoregressively**, meaning that each token is predicted based only on the previously generated tokens and the encoded input sequence.  To achieve this, we employ a technique called **causal masking** (also known as **look-ahead masking**) within the self-attention mechanism.\n",
        "\n",
        "The core idea behind causal masking is simple: **prevent the model from \"peeking\" at future tokens during training.** We want the model to learn to predict the next token based solely on the information available up to the current position in the sequence.  This is accomplished by applying a mask to the attention weights, effectively setting the weights for future tokens to zero (or a very large negative value before the softmax).\n",
        "\n",
        "This masking is essential because, during inference, the model generates the sequence one token at a time. It doesn't have access to future tokens, so it must rely only on its past predictions and the encoded input. By enforcing this constraint during training with causal masking, we ensure that the model learns to generate sequences in a truly autoregressive manner, making it suitable for realistic inference scenarios where future information is unavailable. The following class `MaskedSelfAttention` implements this masking mechanism."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Illustrating the causal mask\n",
        "\n",
        "sequence_length = 4  # You can change this to any desired sequence length\n",
        "\n",
        "# Create the causal mask using jnp.tril\n",
        "causal_mask = torch.tril(\n",
        "    torch.ones((sequence_length, sequence_length), dtype=torch.bool)\n",
        ")\n",
        "\n",
        "print(causal_mask)"
      ],
      "metadata": {
        "id": "mp6I8rteZVA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add causal masks to the model"
      ],
      "metadata": {
        "id": "o1999QvmZz4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MaskedSelfAttention"
      ],
      "metadata": {
        "id": "wE935x41d0Nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedSelfAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_dim):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "\n",
        "    # Calculate query, key, and value matrices using linear layers\n",
        "    self.query = nn.Linear(embed_dim, embed_dim)\n",
        "    self.key = nn.Linear(embed_dim, embed_dim)\n",
        "    self.value = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    # Calculate query, key, and value matrices\n",
        "    query = self.query(x)\n",
        "    key = self.key(x)\n",
        "    value = self.value(x)\n",
        "\n",
        "    # Calculate attention scores (scaled dot-product attention)\n",
        "    attention_scores = torch.matmul(query, key.transpose(1, 2)) / math.sqrt(\n",
        "        self.embed_dim\n",
        "    )\n",
        "\n",
        "    # New  - we add the mask\n",
        "    if mask is not None:\n",
        "      pass\n",
        "      # --- YOUR CODE START ---\n",
        "      # attention_scores = ...\n",
        "      # --- YOUR CODE END ---\n",
        "\n",
        "    attention_weights = nn.functional.softmax(attention_scores, dim=-1)\n",
        "\n",
        "    # Apply attention weights to values\n",
        "    output = torch.matmul(attention_weights, value)\n",
        "    return output"
      ],
      "metadata": {
        "id": "PNL1vORBby6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MaskedTransformerDecoderBlock"
      ],
      "metadata": {
        "id": "YenaT3Msd2Dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedTransformerDecoderBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_dim, mlp_dim):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.mlp_dim = mlp_dim\n",
        "\n",
        "    # Self-attention layer\n",
        "    self.self_attention = MaskedSelfAttention(embed_dim)  # Change\n",
        "    self.cross_attention = CrossAttention(embed_dim)\n",
        "\n",
        "    # Layer normalization\n",
        "    self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    # MLP layers\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(embed_dim, mlp_dim), nn.ReLU(), nn.Linear(mlp_dim, embed_dim)\n",
        "    )\n",
        "\n",
        "    # Layer normalization\n",
        "    self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "  def forward(self, enc_emb, dec_emb, mask):  # Change\n",
        "    x = self.self_attention(dec_emb, mask)  # Change\n",
        "    x = self.layer_norm1(x)\n",
        "\n",
        "    y = self.cross_attention(x, dec_emb)\n",
        "    y = self.layer_norm1(y)\n",
        "\n",
        "    # MLP layer\n",
        "    y = self.mlp(y)\n",
        "    # Layer norm\n",
        "    y = self.layer_norm2(y)\n",
        "\n",
        "    return y"
      ],
      "metadata": {
        "id": "BcIfoMI_du1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MaskedTransformerDecoder"
      ],
      "metadata": {
        "id": "JkQ9XYHDfBE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedTransformerDecoder(nn.Module):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      *,\n",
        "      vocab_size,\n",
        "      max_seq_length,\n",
        "      num_layers,\n",
        "      mlp_dim=256,\n",
        "      embed_dim=128,\n",
        "      num_classes=2,\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.mlp_dim = mlp_dim\n",
        "    self.num_classes = num_classes\n",
        "    self.max_seq_length = max_seq_length\n",
        "    self.num_layers = num_layers\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "    # Embedding layer\n",
        "    self.embedding = nn.Embedding(vocab_size + 1, embed_dim)\n",
        "\n",
        "    # Positional embedding layer\n",
        "    self.pos_embedding = nn.Embedding(max_seq_length, embed_dim)\n",
        "\n",
        "    # Self-attention layer\n",
        "    self.self_attention = SelfAttention(embed_dim)\n",
        "\n",
        "    # Transformer Encoder Blocks\n",
        "    self.transformer_blocks = nn.ModuleList([\n",
        "        MaskedTransformerDecoderBlock(embed_dim, mlp_dim)\n",
        "        for _ in range(num_layers)\n",
        "    ])  # Change\n",
        "\n",
        "  def forward(\n",
        "      self, encoded: torch.Tensor, targets: torch.Tensor\n",
        "  ) -> torch.Tensor:\n",
        "    x = shift_right(targets, self.vocab_size)\n",
        "\n",
        "    # Embedding layer\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # Positional embeddings\n",
        "    positions = torch.arange(x.shape[1], device=x.device)\n",
        "    pos_embeddings = self.pos_embedding(positions)\n",
        "    x = x + pos_embeddings / math.sqrt(self.embed_dim)\n",
        "\n",
        "    # Change : Add a causal mask\n",
        "    batch_size, output_sequence_length, embedding_size = x.shape\n",
        "\n",
        "    # --- YOUR CODE START ---\n",
        "    # causal_mask = ...\n",
        "    # causal_mask = causal_mask.to(...)\n",
        "    # --- YOUR CODE END ---\n",
        "\n",
        "    # Stack multiple transformer encoder blocks\n",
        "    for block in self.transformer_blocks:\n",
        "      x = block(encoded, x, causal_mask)  # Change\n",
        "    return x"
      ],
      "metadata": {
        "id": "PFgKao2zcLS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MaskedTransformer"
      ],
      "metadata": {
        "id": "ZeUrTIirfLxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedTransformer(nn.Module):\n",
        "  \"\"\"Transformer model for sequence-to-sequence tasks.\n",
        "\n",
        "  This class combines the encoder and decoder modules to create a complete\n",
        "  Transformer model.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      num_layers: int,\n",
        "      max_seq_length: int,\n",
        "      vocab_size: int,\n",
        "      embed_dim: int = 32,\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.num_layers = num_layers\n",
        "    self.embed_dim = embed_dim\n",
        "    self.max_seq_length = max_seq_length\n",
        "\n",
        "    # Create an instance of the TransformerEncoder.\n",
        "    self.encoder = BaseTransformerEncoder(\n",
        "        max_seq_length=self.max_seq_length,\n",
        "        num_layers=self.num_layers,\n",
        "        embed_dim=self.embed_dim,\n",
        "        vocab_size=self.vocab_size,\n",
        "    )\n",
        "    # Create an instance of the MaskedTransformerDecoder.\n",
        "    self.decoder = MaskedTransformerDecoder(  # Change\n",
        "        vocab_size=self.vocab_size,\n",
        "        num_layers=self.num_layers,\n",
        "        embed_dim=self.embed_dim,\n",
        "        max_seq_length=self.max_seq_length,\n",
        "    )\n",
        "    # Create a dense layer to project the decoder output to the vocabulary space.\n",
        "    self.linear_output = nn.Linear(self.embed_dim, self.vocab_size)\n",
        "\n",
        "  def forward(\n",
        "      self, inputs: torch.Tensor, targets: torch.Tensor, sample: bool\n",
        "  ) -> torch.Tensor:\n",
        "    del sample  # For now it's a dummy variable, we'll use it later.\n",
        "\n",
        "    # Encode the input sequence.\n",
        "    encoder_output = self.encoder(inputs)\n",
        "\n",
        "    # Decode the encoded input, using the target sequence for teacher forcing.\n",
        "    decoder_output = self.decoder(encoder_output, targets)\n",
        "\n",
        "    # Project the decoder output to the vocabulary space to get logits.\n",
        "    logits = self.linear_output(decoder_output)\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "3My0o1cCfS6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "WrE9wcYNZt8y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmYMO4x61hEu"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "AutoregressiveTransformer = make_transformer_autoregressive(MaskedTransformer)\n",
        "model = AutoregressiveTransformer(\n",
        "    num_layers=1,\n",
        "    vocab_size=task.vocab_size,\n",
        "    max_seq_length=MAX_TEST_LENGTH + 2,\n",
        ")\n",
        "\n",
        "# Run the training loop.\n",
        "df_train, params, tokenizer = run_training(\n",
        "    max_sequence_length=MAX_TRAIN_LENGTH,\n",
        "    task=task,\n",
        "    model=model,\n",
        "    batch_size=128,\n",
        "    train_steps=2_500,\n",
        ")\n",
        "\n",
        "# Visualize the training accuracy.\n",
        "sns.lineplot(data=df_train, x=\"step\", y=\"train_accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eval"
      ],
      "metadata": {
        "id": "s0NoOrW8Z-K7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XR-NkLMGp0J3"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "df_eval = run_evaluation(\n",
        "    model=model,\n",
        "    params=params,\n",
        "    tokenizer=tokenizer,\n",
        "    task=task,\n",
        "    max_test_length=20,\n",
        "    is_autoregressive=True,\n",
        ")\n",
        "\n",
        "# Save the eval data for later comparisons across experiments\n",
        "df_eval_dict[\"masked_autoregressive\"] = df_eval\n",
        "\n",
        "# Plot the test accuracy for each length\n",
        "sns.lineplot(data=df_eval, x=\"length\", y=\"accuracy\", marker=\"o\").set_ylim(\n",
        "    -0.05, 1.05\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis"
      ],
      "metadata": {
        "id": "A2ZsOMYeaB9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_eval_all = pd.concat(\n",
        "    df_eval_dict, keys=df_eval_dict.keys(), names=[\"experiment\", None]\n",
        ")\n",
        "sns.lineplot(\n",
        "    data=df_eval_all, x=\"length\", y=\"accuracy\", hue=\"experiment\", marker=\"o\"\n",
        ").set_ylim(-0.05, 1.05)"
      ],
      "metadata": {
        "id": "-VcFo-KM4dhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Given that, at train time, the model was trained not to peek into the future tokens, at test time it leads to significant improvement on accuracy for autoregressive eval."
      ],
      "metadata": {
        "id": "uyd_3qIp7wpD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VfGtKRerXjh"
      },
      "source": [
        "## Positional embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Motivation"
      ],
      "metadata": {
        "id": "Xc_hz5wIu932"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrYoW4iO2VY5"
      },
      "source": [
        "**Improving Generalization with Sinusoidal Positional Encodings**\n",
        "\n",
        "In Part III, we explored the use of positional embeddings and saw how they significantly impacted the model's ability to generalize to longer sequences. Recall that these embeddings provide the Transformer model with crucial information about the order of tokens within a sequence, as the architecture itself doesn't inherently capture positional relationships.  In our previous implementation, we used randomly initialized, learnable positional embeddings.\n",
        "\n",
        "In this section, we introduce a different type of positional encoding: **sinusoidal positional encodings**. While the motivation remains the same  to inform the model about token positions  sinusoidal encodings offer a deterministic and potentially more effective way to achieve this. Instead of learning a unique embedding for each position, sinusoidal encodings are generated using a predefined mathematical function based on sine and cosine waves with varying frequencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUyx7b8brXFn"
      },
      "outputs": [],
      "source": [
        "def sinusoid_position_encoding(\n",
        "    sequence_length: int,\n",
        "    hidden_size: int,\n",
        "    memory_length: int = 0,\n",
        "    max_timescale: float = 1e4,\n",
        "    min_timescale: float = 2.0,\n",
        "    clamp_length: int = 0,\n",
        "    causal: bool = False,\n",
        "):\n",
        "  \"\"\"Creates sinusoidal positional encodings.\n",
        "\n",
        "  These encodings are used to provide positional information to the Transformer\n",
        "  model,\n",
        "  as it doesn't have inherent mechanisms to understand the order of tokens in a\n",
        "  sequence.\n",
        "\n",
        "  Args:\n",
        "    sequence_length: `int` sequence length (L).\n",
        "    hidden_size: `int` dimension of the positional encoding vectors (D). This is\n",
        "      usually the same as the embedding dimension of the model.\n",
        "    memory_length: `int` size of the memory (M). This is used for models like\n",
        "      Transformer-XL.\n",
        "    max_timescale: `float` maximum timescale for the frequency.\n",
        "    min_timescale: `float` minimum timescale for the frequency.\n",
        "    clamp_length: If greater than 0, any positions further apart than\n",
        "      `clamp_length` are clamped to this value. This is used to limit the\n",
        "      distance between tokens.\n",
        "    causal: If true, then generates a smaller set (L vs 2 * L) of time-encodings\n",
        "      for the use-case of causal attention (e.g., in the decoder).\n",
        "\n",
        "  Returns:\n",
        "    A NumPy array of shape [L + M, D] for causal and [2 * L + M, D] otherwise,\n",
        "    representing the positional encodings.\n",
        "  \"\"\"\n",
        "  # Calculate frequencies for each dimension.\n",
        "  freqs = np.arange(0, hidden_size, min_timescale)\n",
        "  inv_freq = max_timescale ** (-freqs / hidden_size)\n",
        "\n",
        "  # Create the position sequence.\n",
        "  if causal:\n",
        "    # For causal models, we only need encodings for positions up to sequence_length + memory_length.\n",
        "    pos_seq = np.arange(sequence_length + memory_length, 0, -1.0)\n",
        "  else:\n",
        "    # For non-causal models, we need encodings for positions from -(sequence_length + memory_length) to sequence_length.\n",
        "    pos_seq = np.arange(sequence_length + memory_length, -sequence_length, -1.0)\n",
        "\n",
        "  # Clamp positions if clamp_length is specified.\n",
        "  if clamp_length:\n",
        "    pos_seq = np.clip(pos_seq, a_min=-clamp_length, a_max=clamp_length)\n",
        "\n",
        "  # Calculate the sinusoidal inputs.\n",
        "  sinusoid_inp = np.einsum('i,j->ij', pos_seq, inv_freq)\n",
        "\n",
        "  # Create the positional encodings by concatenating sine and cosine values.\n",
        "  pos_emb = np.concatenate(\n",
        "      [np.sin(sinusoid_inp), np.cos(sinusoid_inp)], axis=-1\n",
        "  )\n",
        "\n",
        "  return pos_emb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize the positional embeddings"
      ],
      "metadata": {
        "id": "W1nzvQMPccVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the positional encodings\n",
        "sequence_length = 50\n",
        "hidden_size = 64\n",
        "positional_encodings = sinusoid_position_encoding(sequence_length, hidden_size)\n",
        "\n",
        "# Visualize the positional encodings\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.pcolormesh(positional_encodings, cmap='RdBu')\n",
        "plt.xlabel('Hidden Dimension')\n",
        "plt.xlim((0, hidden_size))\n",
        "plt.ylabel('Sequence Position')\n",
        "plt.title('Sinusoidal Positional Encodings')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9TZcORsOcgUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize individual dimensions to see the waves\n",
        "plt.figure(figsize=(10, 4))\n",
        "for i in range(4):  # Visualize the first 4 dimensions\n",
        "  plt.plot(positional_encodings[:, i], label=f'Dimension {i}')\n",
        "plt.xlabel('Sequence Position')\n",
        "plt.ylabel('Encoding Value')\n",
        "plt.title('Individual Dimensions of Sinusoidal Positional Encodings')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "02CYohc0ctZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add the positional embeddings to the model"
      ],
      "metadata": {
        "id": "15v9Oaf8xqlN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FullTransformerEncoder"
      ],
      "metadata": {
        "id": "tcU7liZVkmpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FullTransformerEncoder(nn.Module):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      *,\n",
        "      vocab_size,\n",
        "      max_seq_length,\n",
        "      num_layers,\n",
        "      mlp_dim=256,\n",
        "      embed_dim=128,\n",
        "      num_classes=2,\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.mlp_dim = mlp_dim\n",
        "    self.num_classes = num_classes\n",
        "    self.max_seq_length = max_seq_length\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # Embedding layer\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "    # Self-attention layer\n",
        "    self.self_attention = SelfAttention(embed_dim)\n",
        "\n",
        "    # Transformer Encoder Blocks\n",
        "    self.transformer_blocks = nn.ModuleList(\n",
        "        [TransformerEncoderBlock(embed_dim, mlp_dim) for _ in range(num_layers)]\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Embedding layer\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    ### Change to sinusoidal positional embeddings\n",
        "    batch_size, sequence_length, embedding_size = x.shape\n",
        "    pos_encodings = sinusoid_position_encoding(\n",
        "        sequence_length=sequence_length,\n",
        "        hidden_size=self.embed_dim,\n",
        "        memory_length=0,\n",
        "        max_timescale=10_000,\n",
        "        min_timescale=2,\n",
        "        clamp_length=0,\n",
        "        causal=True,\n",
        "    )\n",
        "    pos_encodings = torch.Tensor(pos_encodings).to(x.device)\n",
        "    h = x + pos_encodings\n",
        "    ###\n",
        "\n",
        "    # Stack multiple transformer encoder blocks\n",
        "    for block in self.transformer_blocks:\n",
        "      x = block(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "l-pilYG5kENb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FullTransformerDecoder"
      ],
      "metadata": {
        "id": "rZ5S80g8koVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FullTransformerDecoder(nn.Module):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      *,\n",
        "      vocab_size,\n",
        "      max_seq_length,\n",
        "      num_layers,\n",
        "      mlp_dim=256,\n",
        "      embed_dim=128,\n",
        "      num_classes=2,\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.mlp_dim = mlp_dim\n",
        "    self.num_classes = num_classes\n",
        "    self.max_seq_length = max_seq_length\n",
        "    self.num_layers = num_layers\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "    # Embedding layer\n",
        "    self.embedding = nn.Embedding(vocab_size + 1, embed_dim)\n",
        "\n",
        "    # Self-attention layer\n",
        "    self.self_attention = SelfAttention(embed_dim)\n",
        "\n",
        "    # Transformer Encoder Blocks\n",
        "    self.transformer_blocks = nn.ModuleList([\n",
        "        MaskedTransformerDecoderBlock(embed_dim, mlp_dim)\n",
        "        for _ in range(num_layers)\n",
        "    ])\n",
        "\n",
        "  def forward(\n",
        "      self, encoded: torch.Tensor, targets: torch.Tensor\n",
        "  ) -> torch.Tensor:\n",
        "    x = shift_right(targets, self.vocab_size)\n",
        "\n",
        "    # Embedding layer\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    ### Change to sinusoidal positional embeddings\n",
        "    batch_size, sequence_length, embedding_size = x.shape\n",
        "    pos_encodings = sinusoid_position_encoding(\n",
        "        sequence_length=sequence_length,\n",
        "        hidden_size=self.embed_dim,\n",
        "        memory_length=0,\n",
        "        max_timescale=10_000,\n",
        "        min_timescale=2,\n",
        "        clamp_length=0,\n",
        "        causal=True,\n",
        "    )\n",
        "    pos_encodings = torch.Tensor(pos_encodings).to(x.device)\n",
        "    h = x + pos_encodings\n",
        "    ###\n",
        "\n",
        "    batch_size, output_sequence_length, embedding_size = x.shape\n",
        "    causal_mask = torch.tril(\n",
        "        torch.ones(\n",
        "            (batch_size, 1, output_sequence_length, output_sequence_length),\n",
        "            dtype=torch.bool,\n",
        "        )\n",
        "    )\n",
        "    causal_mask = causal_mask.to(encoded.device)\n",
        "\n",
        "    # Stack multiple transformer encoder blocks\n",
        "    for block in self.transformer_blocks:\n",
        "      x = block(encoded, x, causal_mask)\n",
        "    return x"
      ],
      "metadata": {
        "id": "hDcN5TGdkfGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FullTransformer"
      ],
      "metadata": {
        "id": "EP73Hav8lARI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FullTransformer(nn.Module):\n",
        "  \"\"\"Transformer model for sequence-to-sequence tasks.\n",
        "\n",
        "  This class combines the encoder and decoder modules to create a complete\n",
        "  Transformer model.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      num_layers: int,\n",
        "      max_seq_length: int,\n",
        "      vocab_size: int,\n",
        "      embed_dim: int = 32,\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.num_layers = num_layers\n",
        "    self.embed_dim = embed_dim\n",
        "    self.max_seq_length = max_seq_length\n",
        "\n",
        "    # Create an instance of the TransformerEncoder.\n",
        "    self.encoder = FullTransformerEncoder(\n",
        "        max_seq_length=self.max_seq_length,\n",
        "        num_layers=self.num_layers,\n",
        "        embed_dim=self.embed_dim,\n",
        "        vocab_size=self.vocab_size,\n",
        "    )\n",
        "    # Create an instance of the MaskedTransformerDecoder.\n",
        "    self.decoder = FullTransformerDecoder(\n",
        "        vocab_size=self.vocab_size,\n",
        "        num_layers=self.num_layers,\n",
        "        embed_dim=self.embed_dim,\n",
        "        max_seq_length=self.max_seq_length,\n",
        "    )\n",
        "    # Create a dense layer to project the decoder output to the vocabulary space.\n",
        "    self.linear_output = nn.Linear(self.embed_dim, self.vocab_size)\n",
        "\n",
        "  def forward(\n",
        "      self, inputs: torch.Tensor, targets: torch.Tensor, sample: bool\n",
        "  ) -> torch.Tensor:\n",
        "    del sample  # For now it's a dummy variable, we'll use it later.\n",
        "\n",
        "    # Encode the input sequence.\n",
        "    encoder_output = self.encoder(inputs)\n",
        "\n",
        "    # Decode the encoded input, using the target sequence for teacher forcing.\n",
        "    decoder_output = self.decoder(encoder_output, targets)\n",
        "\n",
        "    # Project the decoder output to the vocabulary space to get logits.\n",
        "    logits = self.linear_output(decoder_output)\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "Py95khgIk-BC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "wp1Lkx_Ix1oU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qwjn5SOS275a"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "AutoregressiveTransformer = make_transformer_autoregressive(FullTransformer)\n",
        "model = AutoregressiveTransformer(\n",
        "    num_layers=1,\n",
        "    vocab_size=task.vocab_size,\n",
        "    max_seq_length=MAX_TEST_LENGTH + 2,\n",
        ")\n",
        "\n",
        "# Run the training loop.\n",
        "df_train, params, tokenizer = run_training(\n",
        "    max_sequence_length=MAX_TRAIN_LENGTH,\n",
        "    task=task,\n",
        "    model=model,\n",
        "    batch_size=128,\n",
        "    train_steps=2_500,\n",
        ")\n",
        "\n",
        "# Visualize the training accuracy.\n",
        "sns.lineplot(data=df_train, x=\"step\", y=\"train_accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eval"
      ],
      "metadata": {
        "id": "vPLc_Fnobgy4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyMJpH9dotEt"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "df_eval = run_evaluation(\n",
        "    model=model,\n",
        "    params=params,\n",
        "    tokenizer=tokenizer,\n",
        "    task=task,\n",
        "    max_test_length=20,\n",
        "    is_autoregressive=True,\n",
        ")\n",
        "\n",
        "# Save the eval data for later comparisons across experiments\n",
        "df_eval_dict[\"full_sinusoidal_autoregressive\"] = df_eval\n",
        "\n",
        "# Plot the test accuracy for each length\n",
        "sns.lineplot(data=df_eval, x=\"length\", y=\"accuracy\", marker=\"o\").set_ylim(\n",
        "    -0.05, 1.05\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis"
      ],
      "metadata": {
        "id": "S28d5hSW4fqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_eval_all = pd.concat(\n",
        "    df_eval_dict, keys=df_eval_dict.keys(), names=[\"experiment\", None]\n",
        ")\n",
        "sns.lineplot(\n",
        "    data=df_eval_all, x=\"length\", y=\"accuracy\", hue=\"experiment\", marker=\"o\"\n",
        ").set_ylim(-0.05, 1.05)"
      ],
      "metadata": {
        "id": "AiSA8j3w4hVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In this experiment, sinusoidal positional embeddings lead to a lower accuracy than the learned positional embeddings."
      ],
      "metadata": {
        "id": "V_ZxHf1smgqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ideas of things to try\n",
        "\n",
        "- Play with the hyperparameters and the ordering of modules.\n",
        "- Check to which extend the eval dataset is contaminated with training samples, for the in distribution lengths range. Then fix the issue, to have an in distribution test metric.\n",
        "- Implement other autoregressive sampling methods such as nucleus sampling [1] or beam search [2]\n",
        "- Implement best of n sampling and analyze how the top@n accuracy improves with n.\n",
        "- Implement other positional embeddings such as ROPE [3]\n",
        "- Train the model on real world text datasets\n",
        "- Implement multi-head attention\n",
        "- Look into a lower numerical precision than float32 and analyse how it impacts memory, time and accuracy.\n",
        "\n",
        "**References**\n",
        "- [1] Holtzman, A., Buys, J., Du, L., Forbes, M., & Choi, Y. (2019). The Curious Case of Neural Text Degeneration. ArXiv, abs/1904.09751.\n",
        "- [2] Freitag, Markus and Yaser Al-Onaizan. Beam Search Strategies for Neural Machine Translation. NMT@ACL (2017).\n",
        "- [3] Su, J., Lu, Y., Pan, S., Wen, B., & Liu, Y. (2021). RoFormer: Enhanced Transformer with Rotary Position Embedding. ArXiv, abs/2104.09864."
      ],
      "metadata": {
        "id": "BjoMNkU9mEZV"
      }
    }
  ]
}